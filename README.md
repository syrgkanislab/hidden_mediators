# Detecting clinician implicit biases in diagnoses using proximal causal inference [[paper]](https://psb.stanford.edu/psb-online/proceedings/psb25/liu_k.pdf)

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-%3E%3D3.8-blue.svg)](https://www.python.org/)

## Overview

We provide a user-friendly tool to detect implicit biases in observational datasets.
The main class is `ProximalDE`, which calculates the implicit bias direct effect and provides access to all the auxiliary tests used to validate the result. 

## Table of Contents

- [Installation](#installation)
- [Usage](#usage)
- [Features](#features)
- [Contributing](#contributing)
- [License](#license)
- [Contact](#contact)

## Installation
```
$ git clone https://github.com/syrgkanislab/hidden_mediators
$ cd hidden_mediators
$ pip install -r requirements.txt
```

## Usage

Provide instructions and examples on how to use the project.

```bash
python main.py
```

## Features

- Feature 1: Brief description
- Feature 2: Brief description
- Feature 3: Brief description

## Contributing

Contributions are welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contact

For questions or feedback, please contact:

- Name: Kara Liu
- Email: karaliu [at] stanford . edu
