{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "nJSbD4Nr8VNq"
   },
   "source": [
    "## Setup Only for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26091,
     "status": "ok",
     "timestamp": 1726010690747,
     "user": {
      "displayName": "Vasilis Syrganis",
      "userId": "00968929417250286436"
     },
     "user_tz": 420
    },
    "id": "j4wBu9BNomlq",
    "outputId": "1ec53172-b732-4692-c04f-a326c223664b"
   },
   "outputs": [],
   "source": [
    "# prompt: mount drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1726010691150,
     "user": {
      "displayName": "Vasilis Syrganis",
      "userId": "00968929417250286436"
     },
     "user_tz": 420
    },
    "id": "JfB4MISjovTQ",
    "outputId": "fcfe2ab8-befa-4ac7-c6db-47e2fb56f7f0"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/Colab\\ Notebooks/hidden_mediators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 904,
     "status": "ok",
     "timestamp": 1726010692051,
     "user": {
      "displayName": "Vasilis Syrganis",
      "userId": "00968929417250286436"
     },
     "user_tz": 420
    },
    "id": "D5tm-9Fno5Xn",
    "outputId": "08097071-b875-4577-c8bf-9147c356f9bc"
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "MFy1PyP89gR3"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "VGF-ucGhpC5P"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "!pip install -r requirements.txt\n",
    "time.sleep(2)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "id": "0wsNOzNVtonf"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# replace `develop` with `install` if you wont make library code changes\n",
    "!python setup.py develop\n",
    "time.sleep(2)\n",
    "clear_output()\n",
    "# Restart the session after running this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1726010708657,
     "user": {
      "displayName": "Vasilis Syrganis",
      "userId": "00968929417250286436"
     },
     "user_tz": 420
    },
    "id": "zyL1_6L73Z9e",
    "outputId": "187a0cf7-f7f5-4731-d93b-7ff36c1212de"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/Colab\\ Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "T6FelHd78d8T"
   },
   "source": [
    "# Main Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "a1446bbd-c4be-4846-b09a-e36fad2802ab"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "id": "c79a60f7-dde3-4c7c-8419-ce831b20e824"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from joblib import Parallel, delayed\n",
    "from proximalde.gen_data import gen_data_complex, gen_data_no_controls, gen_data_with_mediator_violations, gen_data_no_controls_discrete_m\n",
    "from proximalde.proximal import proximal_direct_effect, ProximalDE, residualizeW\n",
    "from proximalde.ukbb_proximal import ProximalDE_UKBB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from proximalde.crossfit import fit_predict\n",
    "from proximalde.utilities import covariance, svd_critical_value\n",
    "from proximalde.proximal import residualizeW\n",
    "from proximalde.proxy_rm_utils import *\n",
    "from proximalde.ukbb_data_utils import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Collect results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "ds = ['Low_inc', 'On_dis', 'No_priv_insr', 'No_uni', 'Female', 'Black', 'Obese', 'Asian']\n",
    "ys = ['OA', 'myoc','deprs', 'back', 'RA', 'fibro', 'infl', 'copd','chrkd','mgrn','mela', 'preg', 'endo']\n",
    "dys = list(itertools.product(ds, ys))\n",
    "dys = ['_'.join(x) for x in dys]\n",
    "\n",
    "yy={'mela': 'Melanoma','endo': 'Endometriosis','infl': 'IBD','preg': 'Complications during labor','OA': 'Osteoarthritis','mgrn':'Migraine','copd':'COPD', 'back': 'Back pain', 'deprs': 'Depression', 'myoc': \"Heart disease\", 'RA': 'Rh. Arthritis', 'fibro': 'Fibromyalgia', 'chrkd': 'Chronic kidney disease'}\n",
    "dd={'No_priv_insr': 'Not on private insr.','No_uni': 'No p.s. education', 'Low_inc': 'Low income','Obese':'Obese', 'Female': 'Female', 'Black': 'Black', 'Asian': \"Asian\", 'On_dis': 'Disability insurance'}\n",
    "dys_main = ['Female_myoc','Asian_OA', 'Low_inc_deprs', 'On_dis_RA',  'Obese_OA', 'Black_chrkd']\n",
    "\n",
    "X, X_feats, Z, Z_feats = load_ukbb_XZ_data()\n",
    "Xint = get_int_feats(X_feats)\n",
    "Zint_ = get_int_feats(Z_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median_item(y):\n",
    "    srt_idx = np.argsort(y)\n",
    "    if len(y) % 2 == 0: # if even elements, get max point\n",
    "        i1 = len(y)//2 - 1\n",
    "        i2 = len(y)//2\n",
    "        if np.abs(y[srt_idx[i1]]) > np.abs(y[srt_idx[i2]]):\n",
    "            return y[srt_idx[i1]], srt_idx[i1]\n",
    "        else:\n",
    "            return y[srt_idx[i2]], srt_idx[i2]\n",
    "    else:\n",
    "        return y[srt_idx[len(y)//2]], srt_idx[len(y)//2]\n",
    "\n",
    "def rmNaZ(Zres, Zint):\n",
    "    bad_idx = np.array([('Do not know' in x) or ('Prefer not to' in x) for x in Zint])\n",
    "    Zres = Zres[:,~bad_idx]\n",
    "    Zint = Zint[~bad_idx]\n",
    "    return Zres, Zint\n",
    "\n",
    "\n",
    "def run_inf_rm(D_label, Y_label, inf_idxs, Xset, Zset, save_dir):\n",
    "    np.random.seed(4)\n",
    "    W, _, W_feats, X, X_binary, X_feats, Z, Z_binary, Z_feats, Y, D = load_ukbb_data(D_label=d, Y_label=y)\n",
    "    Z = Z[:,~bad_idx][:,Zset]\n",
    "    X = X[:,Xset]\n",
    "    est = ProximalDE_UKBB(binary_D=False, semi=True, cv=3, verbose=1, random_state=3)\n",
    "    est.fit(np.delete(W, inf_idxs, axis=0), np.delete(D, inf_idxs, axis=0),\n",
    "             np.delete(Z, inf_idxs, axis=0), np.delete(X, inf_idxs, axis=0),\n",
    "             np.delete(Y, inf_idxs, axis=0), D_label=D_label, Y_label=Y_label, save_fname_addn=f'_infRm_{D_label}{Y_label}') \n",
    "    return est.summary(alpha=0.05, save_dir=save_dir,save_fname_addn='_infRm')\n",
    "\n",
    "\n",
    "def run_bootstrap(D_label, Y_label, Xset, Zset, save_dir):\n",
    "    np.random.seed(4)\n",
    "    X, X_feats, Z, Z_feats = load_ukbb_XZ_data()\n",
    "    Xint = get_int_feats(X_feats)\n",
    "    Zint_ = get_int_feats(Z_feats)\n",
    "    bad_idx = np.array([('Do not know' in x) or ('Prefer not to' in x) for x in Zint_])\n",
    "\n",
    "\n",
    "    W, _, W_feats, X, X_binary, X_feats, Z, Z_binary, Z_feats, Y, D = load_ukbb_data(D_label=d, Y_label=y)\n",
    "    est = ProximalDE_UKBB(binary_D=False, semi=True, cv=3, verbose=1, random_state=3)\n",
    "    est.fit(W, D, Z, X, Y, D_label, Y_label, Xset=Xset, Zset=Zset, bad_idx=bad_idx)\n",
    "\n",
    "    inf1 = est.bootstrap_inference(stage=1, n_subsamples=10, fraction=0.5, replace=False, verbose=3, random_state=123)\n",
    "    inf1.summary(save_dir=save_dir, save_fname_addn='_bs_stage1')\n",
    "\n",
    "    inf2 = est.bootstrap_inference(stage=2, n_subsamples=100, fraction=0.5, replace=False, verbose=3, random_state=123)\n",
    "    inf2.summary(save_dir=save_dir, save_fname_addn='_bs_stage2')\n",
    "\n",
    "    inf3 = est.bootstrap_inference(stage=3, n_subsamples=1000, fraction=0.5, replace=False, verbose=3, random_state=123)\n",
    "    inf3.summary(save_dir=save_dir, save_fname_addn='_bs_stage3')\n",
    "    return {'inf1': inf1, 'inf2': inf2, 'inf3':inf3}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Collects all proxy rm sets that pass 4/4 tests\n",
    "could be many per dy pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk \n",
    "\n",
    "def get_cand_and_len(dir, exp_path):\n",
    "    cand_exists = os.path.exists(f'{dir}/{exp_path}/candidates.pkl')\n",
    "    if cand_exists:\n",
    "        cand = pk.load(open(f'{dir}/{exp_path}/candidates.pkl','rb'))\n",
    "        return cand, len(cand)\n",
    "    else:\n",
    "        return [], -1\n",
    "    \n",
    "verbose = True\n",
    "ss_dy = {}\n",
    "for dy in dys:\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\"*2,dy)\n",
    "    dir = f'./results/proxyrm/{dy}'\n",
    "    if not os.path.exists(dir):\n",
    "        continue\n",
    "    all_dy_hparams = [x for x in os.listdir(dir) if 'rm' in x] #todo: delete all other folders\n",
    "\n",
    "    for exp_path in all_dy_hparams:\n",
    "        candidates, n_pairs = get_cand_and_len(dir, exp_path) \n",
    "        if verbose and n_pairs < 1:\n",
    "            print(f\"{exp_path} has {n_pairs} candidate pairs found\")\n",
    "        single_cand_paths = [x for x in os.listdir(f'{dir}/{exp_path}') if not '.pkl' in x]\n",
    "        if n_pairs > 0:\n",
    "            print(exp_path)\n",
    "            unfound = True\n",
    "            for cand_idx in single_cand_paths:\n",
    "                test = pd.read_csv(f'{dir}/{exp_path}/{cand_idx}/table2.csv',header=1, index_col=1)\n",
    "                if test['pass test'].sum() == 4:\n",
    "                    if dy not in ss_dy:\n",
    "                        ss_dy[dy] = []\n",
    "                    point = pd.read_csv(f'{dir}/{exp_path}/{cand_idx}/table0.csv',header=1, index_col=1)\n",
    "                    inf = np.load(f'{dir}/{exp_path}/{cand_idx}/inf_set.npy')\n",
    "                    candidate = candidates[int(cand_idx)]\n",
    "                    ss_dy[dy].append([point, test, inf, f'{dir}/{exp_path}/{cand_idx}', candidate])\n",
    "                    if verbose and unfound:\n",
    "                        print(f\"{exp_path} found 4/4 passing!\")\n",
    "                        unfound = False\n",
    "pk.dump(ss_dy,open('ss_dy.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Recollect / update inf set to be for switching the sign to 0\n",
    "as is, inf set is for alpha=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_dy=pk.load(open('ss_dy.pkl', 'rb'))\n",
    "for dy in ss_dy.keys():\n",
    "    points= np.array([x[0].point.iloc[0] for x in ss_dy[dy]])\n",
    "    m, idx = get_median_item(points)\n",
    "    print(dy, idx)\n",
    "    point, test, inf_idxs, path, (Xset, Zset) = ss_dy[dy][idx]\n",
    "    d, y = '_'.join(dy.split('_')[:-1]), dy.split('_')[-1]\n",
    "    final_ss_dy[dy]  = ss_dy[dy][idx]\n",
    "    Xres, Zres, Yres, Dres = load_ukbb_res_data(d, y)\n",
    "    Zres, Zint = rmNaZ(Zres, Zint_)\n",
    "\n",
    "    est = ProximalDE(semi=True, cv=3, verbose=1, random_state=3)\n",
    "    est.fit(None, Dres, Zres[:, Zset], Xres[:, Xset], Yres)\n",
    "    sm = est.summary()\n",
    "    diag = est.run_diagnostics()\n",
    "    inf_mp200 = est.influential_set(max_points=200)\n",
    "    inf_alhpa = inf_idxs\n",
    "    inf = est.influential_set()\n",
    "    infs = {'switch_sign': inf, 'alpha=.05': inf_idxs, 'n=200':inf_mp200}\n",
    "    for k,v in infs.items():\n",
    "        print(v.shape)\n",
    "    final_ss_dy[dy] = point, test, infs, path, (Xset, Zset)\n",
    "pk.dump(final_ss_dy, open('ss_dy_updated_inf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_dy = pk.load(open('ss_dy_updated_inf.pkl', 'rb'))\n",
    "for dy in ss_dy.keys():\n",
    "        \n",
    "        point, test, inf_dict, path, (Xset, Zset) = ss_dy[dy]\n",
    "        d, y = '_'.join(dy.split('_')[:-1]), dy.split('_')[-1]\n",
    "        Xres, Zres, Yres, Dres = load_ukbb_res_data(d, y)\n",
    "        Zres, Zint = rmNaZ(Zres, Zint_)\n",
    "\n",
    "        est = ProximalDE(semi=True, cv=3, verbose=1, random_state=3)\n",
    "        est.fit(None, Dres, Zres[:, Zset], Xres[:, Xset], Yres)\n",
    "        sm = est.summary()\n",
    "        diag = est.run_diagnostics()\n",
    "\n",
    "        # Inf score / removal \n",
    "        inf_rm_points = []\n",
    "        for name,inds in inf_dict.items():\n",
    "            if 'alpha' not in name:\n",
    "                if not os.path.exists(path + f'/table0_infRm_{name}.csv'):\n",
    "                    run_inf_rm(D_label=d, Y_label=y, inf_idxs=inds, Xset=Xset, Zset=Zset, save_dir=path)\n",
    "                inf_rm_point = pd.read_csv(path + f'/table0_infRm_{name}.csv',header=1, index_col=1)[['point', 'stderr', 'ci_lower', 'ci_upper']]\n",
    "                inf_rm_point['set_size'] = len(inds)\n",
    "                inf_rm_point.columns = [x+f'_infRm_{name}' for x in inf_rm_point.columns]\n",
    "                inf_rm_points.append(inf_rm_point.reset_index())\n",
    "        Run rank test, saving and loading if saved\n",
    "        if os.path.exists(path + '/rank.npy'):\n",
    "            x = np.load(path + '/rank.npy')\n",
    "            svalues, svalues_crit = x[:-1], x[-1]\n",
    "        else: \n",
    "            svalues, svalues_crit = est.covariance_rank_test(calculate_critical=True)\n",
    "            x = np.concatenate([svalues, [svalues_crit]])\n",
    "            np.save(path + '/rank.npy', x)\n",
    "        \n",
    "        # Robust CI to weak iv \n",
    "        lb_robust, ub_robust = est.robust_conf_int(lb=-1, ub=1)\n",
    "\n",
    "        point_df=point\n",
    "        test_df=test\n",
    "        test_df['stat,crit'] = test_df.apply(lambda x: f\"{round(x.statistic,1)},{round(x['critical value'],1)}\", axis=1)\n",
    "        test_df = test_df[['stat,crit','p-value']]\n",
    "        test_df.index=['id','primal','dual', 'weakIV']\n",
    "        test_df_flat = test_df.T.unstack().to_frame().sort_index(level=1).T\n",
    "        test_df_flat.columns = test_df_flat.columns.map('_'.join)\n",
    "        point_df = point_df[['point', 'stderr', 'ci_lower', 'ci_upper']]                  \n",
    "        df = pd.concat([point_df.reset_index(), test_df_flat.reset_index()] + inf_rm_points,axis=1)\n",
    "        point = df[[c for c in df.columns if c != 'index']]\n",
    "        point['ci_lower_robust'] = lb_robust\n",
    "        point['ci_upper_robust'] = ub_robust\n",
    "        point['rank'] = np.sum(svalues >= svalues_crit)\n",
    "        point['Xdim'] = len(Xset)\n",
    "        point['Zdim'] = len(Zset)\n",
    "        point['inf'] = len(inf_dict['switch_sign'])\n",
    "        point['D_Y'] = dy\n",
    "        final_ss_dy_df.append(point)\n",
    "final_ss_dy_df = pd.concat(final_ss_dy_df)\n",
    "final_ss_dy_df.to_csv('all_ss_points.csv')\n",
    "pk.dump(final_ss_dy, open('final_ss_dy.pkl', 'wb'))\n",
    "\n",
    "df = final_ss_dy_df[~final_ss_dy_df.D_Y.isin(dys_main)].reset_index()\n",
    "df['ci'] = (df.ci_upper - df.ci_lower)/2\n",
    "def statcrit_fn(df):\n",
    "    direction = {'dual':'<', 'primal':'<', 'id':'>', 'weakIV':'>'}\n",
    "df['D_Y'] = df.D_Y.map(lambda x: dd['_'.join(x.split('_')[:-1])] + ', ' + yy[x.split('_')[-1]])\n",
    "df['dual_stat,crit'] = df['dual_stat,crit'].map(lambda x: f\"{round(float(x.split(',')[0]),1)}<{round(float(x.split(',')[-1]),1)}\")\n",
    "df['primal_stat,crit'] = df['primal_stat,crit'].map(lambda x: f\"{round(float(x.split(',')[0]),1)}<{round(float(x.split(',')[-1]),1)}\")\n",
    "df['id_stat,crit'] = df['id_stat,crit'].map(lambda x: f\"{round(float(x.split(',')[0]),1)}>{round(float(x.split(',')[-1]),1)}\")\n",
    "df['weakIV_stat,crit'] = df['weakIV_stat,crit'].map(lambda x: f\"{round(float(x.split(',')[0]),1)}>{round(float(x.split(',')[-1]),1)}\")\n",
    "df.point = [f'{round(x,2)}$\\pm${round(y,2)}' for x,y in zip(df.point,df.ci)]\n",
    "df =df[['D_Y', 'point','primal_stat,crit', 'dual_stat,crit', 'id_stat,crit', 'weakIV_stat,crit', 'rank']]\n",
    "df.to_csv('all_main_7metrics.csv',index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_ss_dy_df[final_ss_dy_df.D_Y.isin(dys_main)].reset_index()\n",
    "df['point_robust'] = df.point \n",
    "\n",
    "ci_types = ['','_robust',  '_infRm_n=200','_infRm_switch_sign',]\n",
    "df = df[['D_Y'] + [x+f'{y}' for x in ['point','ci_lower', 'ci_upper'] for y in ci_types]]\n",
    "import seaborn as sns\n",
    "plt.clf()\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    plt.subplots(figsize=(8,8), dpi=100)\n",
    "    df2 = []\n",
    "    for dy in dys_main:\n",
    "    #     sns.set_theme()\n",
    "    #     plt.subplots(figsize=(10,10))\n",
    "        sub_df = df[df.D_Y==dy]\n",
    "        for ci_type in ci_types:\n",
    "            lower = sub_df[f'ci_lower{ci_type}'].iloc[0]\n",
    "            upper = sub_df[f'ci_upper{ci_type}'].iloc[0]\n",
    "            ci_type = {'':'Full data', '_robust': 'Weak IV confidence interval', \n",
    "                        '_infRm_switch_sign': 'Remove full high-influence set',\n",
    "                       '_infRm_n=200': 'Remove top 200 high-influence\\npoints '}[ci_type]\n",
    "            df2.append(pd.DataFrame({'how': [ci_type]*4, 'p': lower, 'points':[lower, lower, upper, upper], 'D_Y': [dy]*4}, index=[0,0,0,0]))\n",
    "    df2 = pd.concat(df2)\n",
    "    df2['D_Y'] = df2.D_Y.map(lambda x: dd['_'.join(x.split('_')[:-1])] + ', ' + yy[x.split('_')[-1]])\n",
    "    ax=sns.boxplot(data=df2, whis=0, showfliers=False, width=.7, linewidth=1.4, x='points', y='D_Y', hue='how')\n",
    "    sns.set_palette(sns.color_palette([\"#B9E5FA\", \"#219EBC\",\"#FB8500\", \"#FFC803\", \"#FAEDCD\",  \"#3C6E71\", ]\n",
    "))\n",
    "    plt.legend(fontsize=12, title='')\n",
    "   \n",
    "    plt.xlabel(\"Effect estimate\")\n",
    "    plt.grid(True, which='both', axis='y', linewidth=0.3, color='gray')\n",
    "    plt.grid(True, which='both', axis='x', linewidth=0.3, color='gray')\n",
    "#     sns.set_context(\"talk\", font_scale=.8)  # You can adjust the font_scale as needed\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "\n",
    "    plt.title(\"Weak IV confidence interval & influence removal test  K\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "plt.clf()\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    plt.subplots(figsize=(15,5), dpi=100)\n",
    "    df2 = []\n",
    "    for dy in df.D_Y.unique():\n",
    "        sns.set_theme()\n",
    "        plt.subplots(figsize=(10,10))\n",
    "        sub_df = df[df.D_Y==dy]\n",
    "        for ci_type in ci_types:\n",
    "            lower = sub_df[f'ci_lower{ci_type}'].iloc[0]\n",
    "            upper = sub_df[f'ci_upper{ci_type}'].iloc[0]\n",
    "            ci_type = {'':'Full data', '_robust': 'Weak IV confidence interval', \n",
    "                        '_infRm_switch_sign': 'Remove full high-influence set',\n",
    "                       '_infRm_n=200': 'Remove top 200 high-influence\\npoints '}[ci_type]\n",
    "            df2.append(pd.DataFrame({'how': [ci_type]*4, 'p': lower, 'points':[lower, lower, upper, upper], 'D_Y': [dy]*4}, index=[0,0,0,0]))\n",
    "    df2 = pd.concat(df2)\n",
    "    df2['D_Y'] = df2.D_Y.map(lambda x: dd['_'.join(x.split('_')[:-1])] + ', ' + yy[x.split('_')[-1]])\n",
    "    df2 = df2.sort_values(by='points')\n",
    "    ax=sns.boxplot(data=df2, whis=0, showfliers=False, width=.7, linewidth=1.4, y='points', x='D_Y', hue='how')\n",
    "    sns.set_palette(sns.color_palette([\"#B9E5FA\", \"#219EBC\",\"#FB8500\", \"#FFC803\", \"#FAEDCD\",  \"#3C6E71\", ]\n",
    "))\n",
    "    plt.legend(fontsize=15, title='')\n",
    "   \n",
    "    plt.xlabel(\"\\$\\theta\\$\")\n",
    "    plt.grid(True, which='both', axis='y', linewidth=0.3, color='gray')\n",
    "    plt.grid(True, which='both', axis='x', linewidth=0.3, color='gray')\n",
    "#     sns.set_context(\"talk\", font_scale=.8)  # You can adjust the font_scale as needed\n",
    "    plt.xticks(fontsize=15,rotation=90)\n",
    "    plt.yticks(fontsize=15)\n",
    "    \n",
    "\n",
    "    plt.title(\"Weak IV confidence interval\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###Stratification res \n",
    "\n",
    "ss_dy = pk.load(open('ss_dy_updated_inf.pkl', 'rb'))\n",
    "W, _, W_feats, X, X_binary, X_feats, Z, Z_binary, Z_feats, Y, D = load_ukbb_data(D_label='Asian', Y_label='OA',pp=True)\n",
    "inc_idx = np.argwhere(['738' in x and '-' not in x for x in W_feats]).flatten()\n",
    "W_feats_int = get_int_feats(W_feats[:-1])\n",
    "strat_dfs  = []\n",
    "for dy in ss_dy.keys():\n",
    "        print(dy)\n",
    "        if 'inc' not in dy and 'endo' not in dy and 'preg' not in dy:\n",
    "            point, test, inf_idxs, path, (Xset, Zset) = ss_dy[dy]\n",
    "            point['dy'] = dy\n",
    "            point['fname'] = 'Full data'\n",
    "            strat_dfs.append(point)\n",
    "\n",
    "            d, y = '_'.join(dy.split('_')[:-1]), dy.split('_')[-1]\n",
    "            Xres, Zres, Yres, Dres = load_ukbb_res_data(d, y)\n",
    "            Zres, Zint = rmNaZ(Zres, Zint_)\n",
    "            est = ProximalDE(semi=True, cv=3, verbose=1, random_state=3)\n",
    "            for i in inc_idx:\n",
    "                idx = (W[:,i]+.5).astype(bool).flatten()\n",
    "                f = W_feats_int[i]\n",
    "                est.fit(None, Dres[idx], Zres[idx][:, Zset], Xres[idx][:, Xset], Yres[idx])\n",
    "                sm = est.summary()\n",
    "                point = pd.DataFrame.from_records(sm.tables[0].data)\n",
    "                cols = point.iloc[0]\n",
    "                point = point.iloc[1:]\n",
    "                point.columns = cols\n",
    "                point['dy'] = dy\n",
    "                point['fname'] = f\n",
    "                strat_dfs.append(point)\n",
    "df = pd.concat(strat_dfs)\n",
    "df.point = df.point.astype(float)\n",
    "df.stderr = df.stderr.astype(float)\n",
    "df = df[df.fname.isin(['Full data',\n",
    "       'Average total household income before tax=Less than 18,000',\n",
    "       'Average total household income before tax=Greater than 100,000'])]\n",
    "df.fname = df.fname.map(lambda x: {'Full data':'Full data',\n",
    "       'Average total household income before tax=Less than 18,000': 'Low income',\n",
    "       'Average total household income before tax=Greater than 100,000': 'High income'}[x])\n",
    "\n",
    "\n",
    "plt.subplots(figsize=(10,20))\n",
    "df_inc_strat = df[~df.dy.isin(['Black,\\nChronic kidney disease','Obese,\\nOsteoarthritis','Asian,\\nOsteoarthritis'])]\n",
    "import seaborn as sns\n",
    "plt.clf()\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    plt.subplots(figsize=(6,20), dpi=100)\n",
    "    df2 = []\n",
    "    df_inc_strat['dy'] = df_inc_strat.dy.map(lambda x: dd['_'.join(x.split('_')[:-1])] + ',\\n' + yy[x.split('_')[-1]])\n",
    "    df_inc_strat['Wint'] = df_inc_strat.fid#.map(lambda x:)\n",
    "    ax=sns.barplot(data=df_inc_strat, x='point', y='dy', hue='fname')\n",
    "    sns.set_palette(sns.color_palette([\"#B9E5FA\", \"#219EBC\",\"#FB8500\", \"#FFC803\", \"#FAEDCD\",  \"#3C6E71\", ]\n",
    "))\n",
    "    plt.legend(fontsize=12, title='')\n",
    "   \n",
    "    plt.xlabel(\"$\\\\theta$\",fontsize=14)\n",
    "    plt.ylabel('')\n",
    "    plt.grid(True, which='both', axis='y', linewidth=0.3, color='gray')\n",
    "    plt.grid(True, which='both', axis='x', linewidth=0.3, color='gray')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=11)\n",
    "\n",
    "    plt.title(\"Stratification by income\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_dy = pk.load(open('ss_dy.pkl', 'rb'))\n",
    "bs_dfs = []\n",
    "for dy in ss_dy.keys():\n",
    "    points= np.array([x[0].point.iloc[0] for x in ss_dy[dy]])\n",
    "    m, idx = get_median_item(points)\n",
    "    if dy in dys_main:\n",
    "        point, test, inf_idxs, path, (Xset, Zset) = ss_dy[dy][idx]\n",
    "        d, y = '_'.join(dy.split('_')[:-1]), dy.split('_')[-1]\n",
    "\n",
    "        \n",
    "        # Subsample / bootstrap work \n",
    "        df_ = point[['point', 'ci_lower', 'ci_upper']].copy()\n",
    "        df_['stage'] = 'All'\n",
    "        df_['frac'] = 'All'\n",
    "        df_['D_Y'] = dy\n",
    "        bs_dfs.append(df_)\n",
    "        \n",
    "        if os.path.exists(path + '/table0_bs_stage3.csv'):\n",
    "            for i in range(1,4):\n",
    "                df_ = pd.read_csv(path + f'/table0_bs_stage{i}.csv',header=1, index_col=1)\n",
    "                df_['stage'] = i\n",
    "                df_['frac'] = float(.5)\n",
    "                df_['D_Y'] = dy\n",
    "                bs_dfs.append(df_)\n",
    "\n",
    "        if os.path.exists(path + '/table0_bs_stage3_nBs1000_frac0.75.csv'):\n",
    "            df_ = pd.read_csv(path + f'/table0_bs_stage3_nBs10000_frac0.25.csv',header=1, index_col=1)\n",
    "            df_['frac'] = float(0.25)\n",
    "            df_['stage'] = 3\n",
    "            df_['D_Y'] = dy\n",
    "            bs_dfs.append(df_)\n",
    "            df_ = pd.read_csv(path + f'/table0_bs_stage3_nBs1000_frac0.75.csv',header=1, index_col=1)\n",
    "            df_['frac'] = float(0.75)\n",
    "            df_['stage'] = 3\n",
    "            df_['D_Y'] = dy\n",
    "            bs_dfs.append(df_)\n",
    "            \n",
    "        if os.path.exists(path + '/table0_bs_stage1_nBs10_frac0.25.csv'):\n",
    "            for frac in ['0.75', '0.25']:\n",
    "                df_ = pd.read_csv(path + f'/table0_bs_stage1_nBs10_frac{frac}.csv',header=1, index_col=1)\n",
    "                df_['frac'] = float(frac)\n",
    "                df_['stage'] = 1\n",
    "                df_['D_Y'] = dy\n",
    "                bs_dfs.append(df_)\n",
    "        else:\n",
    "            print(dy)\n",
    "bs_dfs = pd.concat(bs_dfs)[['point', 'ci_lower', 'ci_upper', 'stage', 'frac', 'D_Y']].reset_index()\n",
    "bs_dfs = bs_dfs[[c for c in bs_dfs.columns if c != 'index']]\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "df = bs_dfs[bs_dfs.frac.isin([.5, 'All'])]\n",
    "df.stage = [{1: '1 (M=10)', 2: '2 (M=100)', 3: '3 (M=1000)', 'All': 'Full data'}[x] for x in df.stage]\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    plt.subplots(figsize=(8,8), dpi=100)\n",
    "    df2 = []\n",
    "    for dy in dys_main:\n",
    "    #     sns.set_theme()\n",
    "    #     plt.subplots(figsize=(10,10))\n",
    "        for stage in df.stage.unique():\n",
    "            sub_df = df[(df.stage==stage) & (df.D_Y==dy)]\n",
    "            lower = sub_df.ci_lower.iloc[0]\n",
    "            upper = sub_df.ci_upper.iloc[0]\n",
    "            dy = sub_df.D_Y.iloc[0]\n",
    "            df2.append(pd.DataFrame({'how': [stage]*4, 'p': lower, 'points':[lower, lower, upper, upper], 'D_Y': [dy]*4}, index=[0,0,0,0]))\n",
    "    df2 = pd.concat(df2)\n",
    "    df2['D_Y'] = df2.D_Y.map(lambda x: dd['_'.join(x.split('_')[:-1])] + ', ' + yy[x.split('_')[-1]])\n",
    "    sns.boxplot(data=df2, whis=0, showfliers=False, width=.7, linewidth=1.4, x='points', y='D_Y', hue='how')\n",
    "    sns.set_palette(sns.color_palette([\"#B9E5FA\", \"#219EBC\",\"#FB8500\", \"#FFC803\", \"#FAEDCD\",  \"#3C6E71\", ]\n",
    "))\n",
    "    plt.legend(fontsize=13, title_fontsize=13, title='Stage of Estimation\\n(M=iterations)', loc='lower left')\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Effect estimate\")\n",
    "    plt.grid(True, which='both', axis='y', linewidth=0.3, color='gray')\n",
    "    plt.grid(True, which='both', axis='x', linewidth=0.3, color='gray')\n",
    "#     sns.set_context(\"talk\", font_scale=.8)  # You can adjust the font_scale as needed\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "\n",
    "    plt.title(\"Comparing stages of re-estimation for bootstrapping       A)   B)\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = bs_dfs[bs_dfs.stage.isin([3, 'All'])]\n",
    "df.frac = [{'All': 'Full data', .5: '50%', .75: '75%', .25: '25%',0.1:'10%' }[x] for x in df.frac]\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    plt.subplots(figsize=(8,8), dpi=100)\n",
    "    df2 = []\n",
    "    for dy in dys_main:\n",
    "    #     sns.set_theme()\n",
    "    #     plt.subplots(figsize=(10,10))\n",
    "        for frac in df.frac.unique():\n",
    "            try:\n",
    "                sub_df = df[(df.frac==frac) & (df.D_Y==dy)]\n",
    "                lower = sub_df.ci_lower.iloc[0]\n",
    "                upper = sub_df.ci_upper.iloc[0]\n",
    "                dy = sub_df.D_Y.iloc[0]\n",
    "                df2.append(pd.DataFrame({'how': [frac]*4, 'p': lower, 'points':[lower, lower, upper, upper], 'D_Y': [dy]*4}, index=[0,0,0,0]))\n",
    "            except Exception as e:\n",
    "                print(dy, frac)\n",
    "    df2 = pd.concat(df2)\n",
    "    custom_order = {'Full data':0, '75%':1, '50%':2, '25%':3}\n",
    "    df2['order'] = df2.how.map(custom_order)\n",
    "    df2=df2.sort_values(by=[ 'order', 'p'])\n",
    "    df2['D_Y'] = df2.D_Y.map(lambda x: dd['_'.join(x.split('_')[:-1])] + ', ' + yy[x.split('_')[-1]])\n",
    "    sns.boxplot(data=df2, whis=0, showfliers=False, width=.7, linewidth=1.4, x='points', y='D_Y', hue='how')\n",
    "    sns.set_palette(sns.color_palette([\"#B9E5FA\", \"#219EBC\",\"#FB8500\", \"#FFB703\", \"#FAEDCD\",  \"#3C6E71\", ]\n",
    "))\n",
    "    plt.legend(fontsize=13,title_fontsize=13, title='Percent of data resampled')\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Effect estimate\")\n",
    "    plt.grid(True, which='both', axis='y', linewidth=0.3, color='gray')\n",
    "    plt.grid(True, which='both', axis='x', linewidth=0.3, color='gray')\n",
    "#     sns.set_context(\"talk\", font_scale=.8)  # You can adjust the font_scale as needed\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "\n",
    "    plt.title(\"Copmaring bootstrap sample size for K=10 iterations\\nRe-estimating at Stage 1\",fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = bs_dfs[bs_dfs.stage.isin([3, 'All'])]\n",
    "df.frac = [{.5: '50%', .75: '75%', .25: '25%', 'All': 'Full data'}[x] for x in df.frac]\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    plt.subplots(figsize=(8,8), dpi=100)\n",
    "    df2 = []\n",
    "    for dy in dys_main:\n",
    "    #     sns.set_theme()\n",
    "    #     plt.subplots(figsize=(10,10))\n",
    "        for frac in df.frac.unique():\n",
    "            try:\n",
    "                sub_df = df[(df.frac==frac) & (df.D_Y==dy)]\n",
    "                lower = sub_df.ci_lower.iloc[0]\n",
    "                upper = sub_df.ci_upper.iloc[0]\n",
    "                dy = sub_df.D_Y.iloc[0]\n",
    "                df2.append(pd.DataFrame({'how': [frac]*4, 'p': lower, 'points':[lower, lower, upper, upper], 'D_Y': [dy]*4}, index=[0,0,0,0]))\n",
    "            except Exception as e:\n",
    "                print(dy, frac)\n",
    "    df2 = pd.concat(df2)\n",
    "    custom_order = {'Full data':0, '75%':1, '50%':2, '25%':3}\n",
    "    df2['order'] = df2.how.map(custom_order)\n",
    "    df2=df2.sort_values(by=[ 'order', 'p'])\n",
    "    df2['D_Y'] = df2.D_Y.map(lambda x: dd['_'.join(x.split('_')[:-1])] + ', ' + yy[x.split('_')[-1]])\n",
    "    sns.boxplot(data=df2, whis=0, showfliers=False, width=.7, linewidth=1.4, x='points', y='D_Y', hue='how')\n",
    "    sns.set_palette(sns.color_palette([\"#B9E5FA\", \"#219EBC\",\"#FB8500\", \"#FFB703\", \"#FAEDCD\",  \"#3C6E71\", ]\n",
    "))\n",
    "    plt.legend(fontsize=13,title_fontsize=13, title='Percent of data resampled')\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Effect estimate\")\n",
    "    plt.grid(True, which='both', axis='y', linewidth=0.3, color='gray')\n",
    "    plt.grid(True, which='both', axis='x', linewidth=0.3, color='gray')\n",
    "#     sns.set_context(\"talk\", font_scale=.8)  # You can adjust the font_scale as needed\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "\n",
    "    plt.title(\"Comparing bootstrap sample size for K=1k iterations\\nRe-estimating at Stage 3\",fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dys_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ukbb_res_data_xgb(D_label, Y_label):\n",
    "    print(\"Assuming D,Y,Z all treated as continuous, using linear regression of W\")\n",
    "    _get_path = lambda fname: f'/oak/stanford/groups/rbaltman/karaliu/bias_detection/causal_analysis/data_hm_std/{fname}'\n",
    "    D_label = D_label.replace('_', '')\n",
    "    save_fname_addn = ''\n",
    "    if Y_label in ['endo', 'preg']:\n",
    "        save_fname_addn='_FemOnly'\n",
    "    Winfo = f'_Wrm{D_label}'\n",
    "    Yres = np.load(_get_path(f'Yres_{Y_label}{Winfo}{save_fname_addn}_Rgrs=xgb.npy')) \n",
    "    Dres = np.load(_get_path(f'Dres_{D_label}{save_fname_addn}_Rgrs=xgb.npy')) \n",
    "    Xres = np.load(_get_path(f'Xres{Winfo}{save_fname_addn}_Rgrs=xgb.npy')) \n",
    "    print(f'Zres{Winfo}{save_fname_addn}_Rgrs=xgb.npy')\n",
    "    Zres = np.load(_get_path(f'Zres{Winfo}{save_fname_addn}_Rgrs=xgb.npy')) \n",
    "    return Xres, Zres, Yres, Dres\n",
    "      \n",
    "final_ss_dy = pk.load(open('ss_dy_updated_inf.pkl', 'rb'))\n",
    "\n",
    "for dy in ['Low_inc_deprs']:#dys_main:\n",
    "    point, test, inf_idxs, path, (Xset, Zset) = final_ss_dy[dy]\n",
    "    print(dy)\n",
    "    d, y = '_'.join(dy.split('_')[:-1]), dy.split('_')[-1]\n",
    "\n",
    "    try:\n",
    "        Xres, Zres, Yres, Dres = load_ukbb_res_data_xgb(d, y)\n",
    "        Zres, Zint = rmNaZ(Zres, Zint_)\n",
    "\n",
    "        est = ProximalDE(semi=True, cv=3, verbose=1, random_state=3)\n",
    "        est.fit(None, Dres, Zres[:, Zset], Xres[:, Xset], Yres)\n",
    "        sm = est.summary()\n",
    "        display(sm.tables[0])\n",
    "        display(sm.tables[2])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "#         diag = est.run_diagnostics()\n",
    "#         inf_mp200 = est.influential_set(max_points=200)\n",
    "#         inf_alhpa = inf_idxs\n",
    "#         inf = est.influential_set()\n",
    "#         infs = {'switch_sign': inf, 'alpha=.05': inf_idxs, 'n=200':inf_mp200}\n",
    "#         for k,v in infs.items():\n",
    "#             print(v.shape)\n",
    "#         final_ss_dy[dy] = point, test, infs, path, (Xset, Zset)\n",
    "# pk.dump(final_ss_dy, open('ss_dy_updated_inf.pkl', 'wb'))\n",
    "# ss_dy = final_ss_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diag.cookd_plot()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diag.l2influence_plot()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proximalde.ukbb_proximal import ProximalDE_UKBB\n",
    "def run_inf_rm(D_label, Y_label, inf_idxs, Xset, Zset, save_dir):\n",
    "    np.random.seed(4)\n",
    "    W, _, W_feats, X, X_binary, X_feats, Z, Z_binary, Z_feats, Y, D = load_ukbb_data(D_label=d, Y_label=y)\n",
    "    Z = Z[:,~bad_idx][:,Zset]\n",
    "    X = X[:,Xset]\n",
    "    est = ProximalDE_UKBB(binary_D=False, semi=True, cv=3, verbose=1, random_state=3)\n",
    "    est.fit(np.delete(W, inf_idxs, axis=0), np.delete(D, inf_idxs, axis=0),\n",
    "             np.delete(Z, inf_idxs, axis=0), np.delete(X, inf_idxs, axis=0),\n",
    "             np.delete(Y, inf_idxs, axis=0), D_label=D_label, Y_label=Y_label, save_fname_addn=f'_infRm_{D_label}{Y_label}') \n",
    "    return est.summary(alpha=0.05, save_dir=save_dir,save_fname_addn='_infRm')\n",
    "# run_inf_rm(D_label=d, Y_label=y, inf_idxs=inds, Xset=Xset, Zset=Zset, save_dir=save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk \n",
    "np.random.seed(30)\n",
    "for D_label in ['Female','Obese','Black', 'Asian']:\n",
    "    for Y_label in tqdm(['OA', 'myoc', 'back']):\n",
    "        dy=f'{D_label}_{Y_label}'\n",
    "        dy = 'Obese_back'\n",
    "        print(\"\\n\"*3)\n",
    "        print(dy)\n",
    "        dir = f'./results/D={D_label}_Y={Y_label}/Dbin=False_Ybin=False_XZbin=False_Rgr=linear/'\n",
    "        test = pd.read_csv(dir + '/table2.csv',header=1, index_col=1)\n",
    "        point = pd.read_csv(dir + '/table0.csv',header=1, index_col=1)\n",
    "        print(\"Estimation on all data:\")\n",
    "        display(point)\n",
    "        display(test)\n",
    "        dir = f'./results/proxyrm/{dy}'\n",
    "        all_paths = os.listdir(dir)\n",
    "        import seaborn as sns\n",
    "        all_data = {}\n",
    "        print(\"Estimations after rm weak proxies (that pass >2 tests):\")\n",
    "        xs = []\n",
    "        zs = []\n",
    "        points = []\n",
    "        for var in all_paths:\n",
    "#             print(var)\n",
    "            candidates = pk.load(open(f'{dir}/{var}/candidates.pkl', 'rb'))\n",
    "            est_paths = [x for x in os.listdir(f'{dir}/{var}') if not '.pkl' in x]\n",
    "            pass_tests = []\n",
    "            for est in est_paths:\n",
    "                test = pd.read_csv(f'{dir}/{var}/{est}/table2.csv',header=1, index_col=1)\n",
    "                if test['pass test'].sum() > 2:\n",
    "                    point = pd.read_csv(f'{dir}/{var}/{est}/table0.csv',header=1, index_col=1)\n",
    "                    xset, zset = candidates[int(est.split('_')[-1])]\n",
    "                    print(len(xset), len(zset))\n",
    "                    xset_ = np.zeros(65)\n",
    "                    zset_ = np.zeros(197)\n",
    "                    xset_[xset] = 1\n",
    "                    zset_[zset] = 1\n",
    "                    xs.append(xset_[None,:])\n",
    "                    zs.append(zset_[None,:])\n",
    "                    display(point)\n",
    "                    display(test)\n",
    "                    print(\"-\"*20)\n",
    "                    points.append(point.point.iloc[0])\n",
    "                pass_tests.append(test['pass test'].sum())\n",
    "#             sns.histplot(pass_tests)\n",
    "#             print((np.array(pass_tests)>2).mean())\n",
    "#             if (np.array(pass_tests)==4).any():\n",
    "#                 print(1/0)\n",
    "#             plt.show()\n",
    "        print(1/0)\n",
    "        dir = f'./results/proxyrm/old_est/'\n",
    "        all_paths = [p for p in os.listdir(dir) if dy in p]\n",
    "        pass_tests = []\n",
    "        for est in all_paths:\n",
    "            try:\n",
    "                test = pd.read_csv(f'{dir}/{est}/table2.csv',header=1, index_col=1)\n",
    "                pass_tests.append(test['pass test'].sum())\n",
    "                if test['pass test'].sum() > 2:\n",
    "                    point = pd.read_csv(f'{dir}/{est}/table0.csv',header=1, index_col=1)\n",
    "                    display(point)\n",
    "                    display(test)\n",
    "#                 if test['pass test'].sum() == 3:\n",
    "#                     print(est)\n",
    "            except:\n",
    "                pass\n",
    "#         print((np.array(pass_tests)>2).mean())\n",
    "#         sns.histplot(pass_tests)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "all_proxyrm_table_paths = os.listdir(f'./results/proxyrm/')\n",
    "all_data = {}\n",
    "for D_label in ['Female','Obese','Black', 'Asian']:\n",
    "    for Y_label in tqdm(['deprs', 'back']):\n",
    "        dir = f'./results/D={D_label}_Y={Y_label}/Dbin=False_Ybin=False_XZbin=False_Rgr=linear/'\n",
    "        test = pd.read_csv(dir + '/table2.csv',header=1, index_col=1)\n",
    "        point = pd.read_csv(dir + '/table0.csv',header=1, index_col=1)\n",
    "\n",
    "#         candidates = pk.load(open(f'./{D_label}_{Y_label}_candidates_reweight_acually.pkl', 'rb'))\n",
    "        candidates = pk.load(open(f'./{D_label}_{Y_label}_candidates_reweight.pkl', 'rb'))\n",
    "        new_tests = []\n",
    "        new_points = []\n",
    "        xsets=[]\n",
    "        zsets=[]\n",
    "        ps=[p for p in all_proxyrm_table_paths if f'{D_label}_{Y_label}' in p and 'Dbin' not in p and 'random' in p]\n",
    "        if len(ps) > 0:\n",
    "            print(dir)\n",
    "            display(point)\n",
    "            display(test)\n",
    "        for path in ps:\n",
    "            try:\n",
    "                point = pd.read_csv(f'./results/proxyrm/{path}/table0.csv', header=1, index_col=1)\n",
    "                test = pd.read_csv(f'./results/proxyrm/{path}/table2.csv', header=1, index_col=1)\n",
    "                i = int(path.split(\"_\")[-1])\n",
    "                Xset, Zset = candidates[i]\n",
    "                if test['pass test'].sum() > 2:\n",
    "                    print(\"----\"*10)\n",
    "                    new_tests.append(test)\n",
    "                    new_points.append(point)\n",
    "                    rmXset = np.setdiff1d(np.arange(Xres.shape[1]), Xset)\n",
    "                    x=Xint[Xset]\n",
    "                    np.random.shuffle(x)\n",
    "                    print(\"Kept Xs = \", x[:10])\n",
    "                    x=Xint[rmXset]\n",
    "                    np.random.shuffle(x)\n",
    "                    print(\"Deleted Xs =\", x)\n",
    "\n",
    "                    rmZset = np.setdiff1d(np.arange(Zres.shape[1]), Zset)\n",
    "                    \n",
    "                    x=Zint[Zset]\n",
    "                    np.random.shuffle(x)\n",
    "                    print(\"Kept Zs = \", x[:10])\n",
    "                    x=Zint[rmZset]\n",
    "                    np.random.shuffle(x)\n",
    "                    print(\"Deleted Zs =\", x[:10])\n",
    "                    x = np.zeros(Xres.shape[1])\n",
    "                    x[Xset] = 1\n",
    "                    z = np.zeros(Zres.shape[1])\n",
    "                    z[Zset] = 1\n",
    "                    xsets.append(x[None,:])\n",
    "                    zsets.append(z[None,:])\n",
    "                    display(test)\n",
    "                    display(point)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        if len(new_tests) > 0:\n",
    "            plt.subplots(figsize=(10,10))\n",
    "            plt.imshow(np.concatenate(xsets))\n",
    "            plt.show()\n",
    "            plt.subplots(figsize=(10,10))\n",
    "            plt.imshow(np.concatenate(zsets))\n",
    "            plt.show()\n",
    "            all_data[f'{D_label}_Y={Y_label}'] = {'tests': [og_test]+new_tests, \n",
    "                                    'point': [og_point] + new_points, 'xsets': xsets,\n",
    "                                                 'zsets':zsets}\n",
    "            print(f'{D_label}_Y={Y_label}', len(new_tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dfs = []\n",
    "res_dfs = []\n",
    "point_dfs = []\n",
    "model_regression = 'linear'\n",
    "model_classification = 'linear'\n",
    "Dbin = False \n",
    "Ybin = False\n",
    "SAVE_PATH = './results/'\n",
    "XZbin = False\n",
    "for D_label in ['Black', 'Female', 'Obese','Asian']:\n",
    "        print(D_label)\n",
    "        for Y_label in ['OA', 'RA', 'myoc','deprs', 'back']:\n",
    "\n",
    "            try:\n",
    "#                     res_model_save = ''\n",
    "#                     if res_model == 'xgb':\n",
    "#                         res_model_save = '_xgb'\n",
    "                save_dir = f'{SAVE_PATH}/D={D_label}_Y={Y_label}/Dbin={Dbin}_Ybin={Ybin}_XZbin={XZbin}_Rgr={model_regression}'\n",
    "                test_df = pd.read_csv(save_dir + '/table2.csv', header=1, index_col=1)\n",
    "                test_df = test_df.drop(columns=['0'])\n",
    "                test_df_flat = test_df.T.unstack().to_frame().sort_index(level=1).T\n",
    "                test_df_flat.columns = test_df_flat.columns.map('_'.join)\n",
    "                point_df = pd.read_csv(save_dir + '/table0.csv', header=1, index_col=1)\n",
    "                point_df = point_df.drop(columns=['0'])                    \n",
    "                res_df = pd.read_csv(save_dir + '/table1.csv', header=1, index_col=1)\n",
    "                res_df = res_df.drop(columns=['0'])\n",
    "                test_df_flat['D_Y'] = point_df['D_Y'] = res_df['D_Y'] = f'{D_label}_{Y_label}'\n",
    "#                 test_df_flat['res_model'] = point_df['res_model'] = res_df['res_model'] = res_model\n",
    "                res_dfs.append(res_df)\n",
    "                point_dfs.append(point_df)\n",
    "                test_dfs.append(test_df_flat)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "point_df = pd.concat(point_dfs)\n",
    "res_df = pd.concat(res_dfs)\n",
    "test_df = pd.concat(test_dfs)\n",
    "test_df = test_df.reindex(sorted(test_df.columns), axis=1)\n",
    "point_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "import textwrap \n",
    "def wrap_labels(labels, max_characters):\n",
    "    return [textwrap.fill(label, max_characters) for label in labels]\n",
    "    \n",
    "UKBB_DATA_DIR = '/oak/stanford/groups/rbaltman/karaliu/bias_detection/cohort_creation/data/'\n",
    "\n",
    "def _load_data(fname: str):\n",
    "    data = np.load(UKBB_DATA_DIR + f'{fname}_data_rd.npy', allow_pickle=False)    \n",
    "    feats = np.load(UKBB_DATA_DIR + f'{fname}_feats_rd.npy', allow_pickle=False)\n",
    "    assert np.isnan(data).sum() == 0, 'NaN values cannot exist in data'\n",
    "    return data, feats\n",
    "    \n",
    "def load_XZ_data():\n",
    "    Z, Z_feats = _load_data(fname = 'srMntSlp')\n",
    "    X, X_feats = _load_data(fname = 'biomMed')\n",
    "    return X, X_feats, Z, Z_feats\n",
    "\n",
    "def load_DY_data(D_label, Y_label):\n",
    "    D_df = pd.read_csv(UKBB_DATA_DIR + 'updated_sa_df_pp.csv')\n",
    "    D = D_df[D_label].to_numpy()     \n",
    "    Y = pd.read_csv(UKBB_DATA_DIR + 'updated_Y_labels.csv')[Y_label].to_numpy()[:,None] \n",
    "    return D, Y\n",
    "\n",
    "def load_res_data(D_label, Y_label):\n",
    "    _get_path = lambda fname: f'/oak/stanford/groups/rbaltman/karaliu/bias_detection/causal_analysis/data_hm/{fname}'\n",
    "    D_label = D_label.replace('_', '')\n",
    "    Winfo = f'_Wrm{D_label}'\n",
    "    Yres = np.load(_get_path(f'Yres_{Y_label}{Winfo}.npy')) \n",
    "    Dres = np.load(_get_path(f'Dres_{D_label}.npy')) \n",
    "    Xres = np.load(_get_path(f'Xres{Winfo}.npy')) \n",
    "    Zres = np.load(_get_path(f'Zres{Winfo}.npy')) \n",
    "    return Xres, Zres, Yres, Dres\n",
    "\n",
    "def XZ_hparam_plot(dual_or_primal='dual'):\n",
    "    \"\"\"\n",
    "    Tool for visualizing covariance matrices \n",
    "    and how different thresholds for N affect the corresponding covariance.\n",
    "    Could be for dual or primal. \n",
    "    \"\"\"\n",
    "    \n",
    "    D_labels = ['Female', 'Obese','Black', 'Asian']\n",
    "    if dual_or_primal=='dual':\n",
    "        Y_labels = ['OA']\n",
    "    else:\n",
    "        Y_labels = ['OA', 'RA', 'myoc', 'copd', 'deprs', 'back']\n",
    "    for D_label in D_labels:\n",
    "        print(D_label)\n",
    "        for Y_label in Y_labels:\n",
    "            Xres, Zres, _, Dres = load_res_data(D_label, Y_label=Y_label)\n",
    "            if dual_or_primal=='dual':\n",
    "                Xrpr, Zrpr, Drpr, label = 'X', 'Z', 'D', D_label\n",
    "                XZres_cov, XZres_pvals, XZres_thresh = get_cov(Xres, Zres, get_pvals=True)\n",
    "                DXres_cov, DXres_pvals, DXres_thresh = get_cov(Dres, Xres, get_pvals=True)\n",
    "            else:\n",
    "                Xrpr, Zrpr, Drpr, label = 'Z', 'X', 'Y', Y_label\n",
    "                XZres_cov, XZres_pvals, XZres_thresh = get_cov(Zres, Xres, get_pvals=True)\n",
    "                DXres_cov, DXres_pvals, DXres_thresh = get_cov(Yres, Zres, get_pvals=True)\n",
    "            \n",
    "            # We only care about X feats with st.sig. assn with D\n",
    "            ss_DXidx = (DXres_pvals < DXres_thresh).squeeze()\n",
    "            if dual_or_primal=='dual':\n",
    "                fig, axs = plt.subplots(1, 2, figsize=(20, 5), dpi=70)\n",
    "            else:\n",
    "                fig, axs = plt.subplots(1, 2, figsize=(12, 5), dpi=70)\n",
    "\n",
    "            im = axs[0].imshow((XZres_pvals[ss_DXidx] < XZres_thresh), aspect='auto', cmap='Blues', interpolation='nearest')\n",
    "            axs[0].set_title(f\"Nonzero Covariance({Xrpr},{Zrpr})\\n(if spearman pvalue < .05/dz*dx)\", fontsize=12)\n",
    "            axs[0].set_ylabel(f\"{Xrpr} feats\", fontsize=10)\n",
    "            axs[0].set_xlabel(f\"{Zrpr} feats\", fontsize=10)\n",
    "            cbar = fig.colorbar(im, ax=axs[0], orientation='vertical')\n",
    "            cbar.set_ticks([0, 1])\n",
    "            cbar.set_ticklabels(['Zero', 'Nonzero'])\n",
    "\n",
    "            \n",
    "            nZfeats = []\n",
    "            Xfeats_w_zero_Zfeats = []\n",
    "            for i in range(40):\n",
    "                keep = (XZres_pvals[ss_DXidx] < XZres_thresh).sum(axis=0) > i\n",
    "                zero = ((XZres_pvals[ss_DXidx][:, keep] < XZres_thresh).sum(axis=1) == 0).sum()\n",
    "                nZfeats.append(keep.sum())\n",
    "                Xfeats_w_zero_Zfeats.append(zero)\n",
    "                \n",
    "            axs[1].set_title(f\"Each {Zrpr} feat's # st.sig. correlations w/ all {Xrpr} feats, {Drpr}={label}\", fontsize=12)\n",
    "            ax3 = axs[1]\n",
    "            ax3.plot(range(40), nZfeats, color='blue')\n",
    "            ax3.set_ylabel(f\"# {Zrpr} feats correlated with >N {Xrpr} feats\", color='blue')\n",
    "            ax3.set_xlabel(f\"N\\n(hparam for filtering {Xrpr} feats)\")\n",
    "            ax3.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "            ax3b = ax3.twinx()\n",
    "            ax3b.plot(range(40), Xfeats_w_zero_Zfeats, color='green')\n",
    "            ax3b.set_ylabel(f\"# {Xrpr} feats w/ 0 st.sig. feat correlation\\nafter rm {Zrpr} feats correlated with <N {Xrpr} feats\", color='green')\n",
    "            ax3b.tick_params(axis='y')\n",
    "\n",
    "            ax3.set_title(f'How removing {Zrpr} feats affects Cov(X,Z)')\n",
    "            ax3.grid(True, axis='y', linestyle='--', linewidth=0.5, color='lightgray')\n",
    "\n",
    "            # Adjust layout to prevent overlapping\n",
    "            plt.tight_layout()\n",
    "            plt.suptitle(f\"{dual_or_primal} violation plots for {Drpr}={label}\\nusing Xres, Zres\")\n",
    "            # Show the combined plots\n",
    "            plt.show()\n",
    "\n",
    "def XZ_vis_cov(rmX_zeroZ_dual, popssZ_dual, rmZ_zeroX_primal, popssX_primal):\n",
    "\n",
    "\n",
    "    for D_label in ['Female', 'Obese','Black', 'Asian']:\n",
    "        for Y_label in ['OA', 'RA', 'myoc', 'copd', 'deprs', 'back']:\n",
    "            print(f\"{D_label}->{Y_label}\")\n",
    "            W, W_feats, X, X_feats, Z, Z_feats, Y, D = load_ukbb_data(D_label=D_label, Y_label=Y_label)\n",
    "            Xres, Zres, Yres, Dres = load_res_data(D_label, Y_label)\n",
    "\n",
    "            # Filter X,Z based on bad proxies proxies \n",
    "            Xprm_idx = popssX_primal[Y_label][D_label]    \n",
    "            Zprm_idx = ~rmZ_zeroX_primal[Y_label][D_label]\n",
    "            Zdual_idx = popssZ_dual['OA'][D_label]   \n",
    "            Xdual_idx = ~rmX_zeroZ_dual['OA'][D_label]\n",
    "\n",
    "            Xres = Xres[:,(Xprm_idx & Xdual_idx)]\n",
    "            Zres = Zres[:,(Zprm_idx & Zdual_idx)]\n",
    "\n",
    "            XZres_cov, XZres_pvals, XZres_thresh = get_cov(Xres,Zres, get_pvals=True)\n",
    "            DXres_cov, DXres_pvals, DXres_thresh = get_cov(Dres, Xres, get_pvals=True)\n",
    "            YZres_cov, YZres_pvals, YZres_thresh = get_cov(Yres, Zres, get_pvals=True)\n",
    "\n",
    "            XZres_cov1 = np.concatenate([XZres_cov, YZres_cov], axis=0)\n",
    "            DXres_cov1 = np.vstack([np.array([[0]]), DXres_cov.reshape(-1, 1)])\n",
    "            XZres_cov2 = np.concatenate([XZres_cov1, DXres_cov1], axis=1)\n",
    "            plt.subplots(1,1,figsize=(12,8),dpi=60)\n",
    "            sns.heatmap(np.abs(XZres_cov2), cmap='Blues')\n",
    "            plt.axhline(y=Xres.shape[1], color='black', linewidth=2)  # Line between N and N+1 (where the vector is appended)\n",
    "            plt.axvline(x=Zres.shape[1], color='black', linewidth=2)  # Line between N and N+1 (where the vector is appended)\n",
    "            xtick_labels = list(Xint[(Xprm_idx & Xdual_idx)]) + [f'D={D_label}']  # First N are xi, last one is D\n",
    "            plt.yticks(ticks=np.arange(Xres.shape[1]+1)+.5, labels=xtick_labels, rotation=0)\n",
    "            ytick_labels = list(Zint[(Zprm_idx & Zdual_idx)]) + [f'Y={Y_label}']  # First N are xi, last one is D\n",
    "            plt.xticks(ticks=np.arange(Zres.shape[1]+1)+.5, labels=wrap_labels(ytick_labels,50),rotation=90, fontsize=8)\n",
    "            plt.xlabel('Z feats')\n",
    "            plt.ylabel('X feats')\n",
    "            plt.title(f'|Cov(X,Z)| after filtering X,Z\\n{D_label}->{Y_label}')\n",
    "            plt.show()\n",
    "\n",
    "            assert ((XZres_pvals < XZres_thresh).sum(axis=0) > 0).all()\n",
    "            assert ((XZres_pvals < XZres_thresh).sum(axis=1) > 0).all()\n",
    "            XZres_cov1 = np.concatenate([XZres_pvals < XZres_thresh, YZres_pvals < YZres_thresh], axis=0)\n",
    "            DXres_cov1 = np.vstack([np.array([[0]]), DXres_pvals.reshape(-1, 1) < DXres_thresh])\n",
    "            XZres_cov2 = np.concatenate([XZres_cov1, DXres_cov1], axis=1)\n",
    "            plt.subplots(1,1,figsize=(12,8),dpi=60)\n",
    "            sns.heatmap(np.abs(XZres_cov2), cmap='Blues')\n",
    "            plt.axhline(y=Xres.shape[1], color='black', linewidth=2)  # Line between N and N+1 (where the vector is appended)\n",
    "            plt.axvline(x=Zres.shape[1], color='black', linewidth=2)  # Line between N and N+1 (where the vector is appended)\n",
    "            xtick_labels = list(Xint[(Xprm_idx & Xdual_idx)]) + [f'D={D_label}']  # First N are xi, last one is D\n",
    "            plt.yticks(ticks=np.arange(Xres.shape[1]+1)+.5, labels=xtick_labels, rotation=0)\n",
    "            ytick_labels = list(Zint[(Zprm_idx & Zdual_idx)]) + [f'Y={Y_label}']  # First N are xi, last one is D\n",
    "            plt.xticks(ticks=np.arange(Zres.shape[1]+1)+.5, labels=wrap_labels(ytick_labels,50),rotation=90, fontsize=8)\n",
    "            plt.xlabel('Z feats')\n",
    "            plt.ylabel('X feats')\n",
    "            plt.title(f'Nonzero Cov(X,Z) after filtering X,Z\\n{D_label}->{Y_label}')\n",
    "            plt.show()\n",
    "\n",
    "            DYXres = np.concatenate([Dres, Yres, Xres], axis=1)\n",
    "            Zall_cov, Zall_pvals, Zall_thresh = get_cov(DYXres, Zres, get_pvals=True)\n",
    "            plt.subplots(1,1,figsize=(12,8),dpi=60)\n",
    "            sns.heatmap(np.abs(Zall_cov), cmap='Blues')\n",
    "            xtick_labels =[f'D={D_label}', f'Y={Y_label}'] + list(Xint[(Xprm_idx & Xdual_idx)])  # First N are xi, last one is D\n",
    "            plt.yticks(ticks=np.arange(Xres.shape[1]+2)+.5, labels=xtick_labels, rotation=0)\n",
    "            ytick_labels = list(Zint[(Zprm_idx & Zdual_idx)])\n",
    "            plt.xticks(ticks=np.arange(Zres.shape[1])+.5, labels=wrap_labels(ytick_labels,50),rotation=90, fontsize=10)\n",
    "            plt.ylabel('X feats')\n",
    "            plt.xlabel('Z feats')\n",
    "            plt.title(f'|Cov(DYX,Z)| after filtering X,Z\\n{D_label}->{Y_label}')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            DYZres = np.concatenate([Dres, Yres, Zres], axis=1)\n",
    "            Zall_cov, Zall_pvals, Zall_thresh = get_cov(DYZres, Xres, get_pvals=True)\n",
    "            plt.subplots(1,1,figsize=(12,8),dpi=60)\n",
    "            sns.heatmap(np.abs(Zall_cov), cmap='Blues')\n",
    "            xtick_labels =[f'D={D_label}', f'Y={Y_label}'] + list(Zint[(Zprm_idx & Zdual_idx)])  # First N are xi, last one is D\n",
    "            plt.yticks(ticks=np.arange(Zres.shape[1]+2)+.5, labels=xtick_labels, rotation=0)\n",
    "            ytick_labels = list(Xint[(Xprm_idx & Xdual_idx)])\n",
    "            plt.xticks(ticks=np.arange(Xres.shape[1])+.5, labels=wrap_labels(ytick_labels,50),rotation=90, fontsize=10)\n",
    "            plt.ylabel('Z feats')\n",
    "            plt.xlabel('X feats')\n",
    "            plt.title(f'|Cov(DYZ,X| after filtering X,Z\\n{D_label}->{Y_label}')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "for i in idx_list = np.random.choice(np.arange(len(candidates)), size=2)\n",
    "for Xset, Zset in candidates[idx_list]:\n",
    "    print(\"Xset =\", Xset)\n",
    "    print(\"Zset =\", Zset)\n",
    "    print()\n",
    "    est = ProximalDE(random_state=3)\n",
    "    est.fit(None, Dres, Zres[:, Zset], Xres[:, Xset], Yres)\n",
    "    t = est.summary().tables[2]\n",
    "    df = pd.DataFrame.from_records(t.data)\n",
    "    header = df.iloc[0] # grab the first row for the header\n",
    "    df = df[1:] # take the data less the header row\n",
    "    df.columns = header\n",
    "    df['pass test'] = df['pass test'].map(lambda x: x == 'True')\n",
    "    if df['pass test'].all():\n",
    "        display(est.summary().tables[0], est.summary().tables[2])\n",
    "        print(\"Xset =\", Xset)\n",
    "        print(\"Zset =\", Zset)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "T6FelHd78d8T",
    "57abc1f7-1569-469d-8239-12fce056e875",
    "a482021b-9c5b-4123-b20c-878b2940ee33",
    "M0Zp1JDoZBbi"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "11wI4lKMKFJmVN9v4WAK15TtifoY686Vb",
     "timestamp": 1726017290265
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
