{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "T6FelHd78d8T"
   },
   "source": [
    "# Main Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "id": "a1446bbd-c4be-4846-b09a-e36fad2802ab"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "c79a60f7-dde3-4c7c-8419-ce831b20e824"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from joblib import Parallel, delayed\n",
    "from proximalde.proximal import proximal_direct_effect, ProximalDE, residualizeW\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from proximalde.crossfit import fit_predict\n",
    "from proximalde.utilities import covariance, svd_critical_value\n",
    "from proximalde.proximal import residualizeW\n",
    "from proximalde.proxy_rm_utils import *\n",
    "from proximalde.ukbb_data_utils import *\n",
    "import seaborn as sns\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median_item(y):\n",
    "    srt_idx = np.argsort(y)\n",
    "    if len(y) % 2 == 0: # if even elements, get max point\n",
    "        i1 = len(y)//2 - 1\n",
    "        i2 = len(y)//2\n",
    "        if np.abs(y[srt_idx[i1]]) > np.abs(y[srt_idx[i2]]):\n",
    "            return y[srt_idx[i1]], srt_idx[i1]\n",
    "        else:\n",
    "            return y[srt_idx[i2]], srt_idx[i2]\n",
    "    else:\n",
    "        return y[srt_idx[len(y)//2]], srt_idx[len(y)//2]\n",
    "\n",
    "X, X_feats, Z, Z_feats = load_ukbb_XZ_data()\n",
    "Xint = get_int_feats(X_feats)\n",
    "Zint_ = get_int_feats(Z_feats)\n",
    "bad_idx = np.array([('Do not know' in x) or ('Prefer not to' in x) for x in Zint_])\n",
    "ss_dy = pk.load(open('ss_dy_updated_inf.pkl', 'rb'))\n",
    "ss_dy.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Inf set heat map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_idxs = []\n",
    "i=0\n",
    "labels = []\n",
    "yy={'mela': 'Melanoma','endo': 'Endometriosis','infl': 'IBD','preg': 'Complications during labor','OA': 'Osteoarthritis','mgrn':'Migraine','copd':'COPD', 'back': 'Back pain', 'deprs': 'Depression', 'myoc': \"Heart disease\", 'RA': 'Rh. Arthritis', 'fibro': 'Fibromyalgia', 'chrkd': 'Chronic kidney disease'}\n",
    "dd={'No_priv_insr': 'Not on private insr.','No_uni': 'No p.s. education', \n",
    "    'Low_inc': 'Low income','Obese':'Obese', 'Female': 'Female', 'Black': 'Black', 'Asian': \"Asian\", \n",
    "    'On_dis': 'Disability insr.'}\n",
    "\n",
    "for k in sorted(ss_dy.keys(), key=lambda x: x.split('_')[-1]):\n",
    "    point, test, inf_dict, path, (Xset, Zset) = ss_dy[k]\n",
    "    inf = inf_dict['switch_sign']\n",
    "    if np.abs(point.point.iloc[0])> 0.01:\n",
    "        bin_idx = np.zeros(502411)\n",
    "        bin_idx[inf] = 1\n",
    "        bin_idxs.append(bin_idx[:,None])\n",
    "        labels.append(f\"{dd['_'.join(k.split('_')[:-1])] + ', ' + yy[k.split('_')[-1]]} ({int(bin_idx.sum())})\")\n",
    "        i += 1\n",
    "    \n",
    "bin_idxs = np.concatenate(bin_idxs,axis=1)\n",
    "bin_idxs = bin_idxs[bin_idxs.sum(axis=1)!=0]\n",
    "\n",
    "sns.heatmap(np.corrcoef(bin_idxs.T),xticklabels=labels, yticklabels=labels,cmap='Blues', annot=False)\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize=(9,7),dpi=100)\n",
    "n = bin_idxs.shape[1]\n",
    "coeff = np.zeros((n,n))\n",
    "for i in range(n):\n",
    "    inf = bin_idxs[:,i].astype(bool)\n",
    "    for j in range(i, n):\n",
    "        inf2 = bin_idxs[:,j].astype(bool)\n",
    "        numer = (inf & inf2).sum()\n",
    "        denom = min(inf.sum(), inf2.sum())\n",
    "#         denom = (inf | inf2).sum()\n",
    "        coeff[i,j] = coeff[j,i] = numer/denom\n",
    "\n",
    "sns.heatmap(coeff,xticklabels=labels, yticklabels=labels,cmap='Blues', linewidth=.2,annot=False), ss_dy.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_dy = pk.load(open('ss_dy_updated_inf.pkl', 'rb'))\n",
    "dy_strs =  ['Low_inc', 'On_dis', 'No_priv_insr', 'No_uni', 'Female', 'Black', 'Obese', 'Asian']+['OA', 'myoc','deprs', 'back', 'RA', 'fibro', 'infl', 'copd','chrkd','mgrn','mela', 'preg', 'endo']\n",
    "dy = 'Obese_OA'\n",
    "point, test, inf_dict, path, (Xset, Zset) = ss_dy[dy]\n",
    "d, y = '_'.join(dy.split('_')[:-1]), dy.split('_')[-1]\n",
    "W, _, W_feats, X, X_binary, X_feats, Z, Z_binary, Z_feats, Y, D = load_ukbb_data(D_label=d, Y_label=y, pp = False)\n",
    "Z = Z[:,~bad_idx][:,Zset]\n",
    "X = X[:,Xset]\n",
    "Z_feats, X_feats = Z_feats[~bad_idx][Zset], X_feats[Xset]\n",
    "data = np.concatenate([W,Z,X,D[:,None],Y], axis=1)\n",
    "feats = np.concatenate([W_feats,Z_feats,X_feats,[d],[y]])\n",
    "FID = np.concatenate([[x.split('.')[1] for x in feats[:-2]],feats[-2:]])\n",
    "data.shape, feats.shape, FID.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_dy = pk.load(open('ss_dy_updated_inf.pkl', 'rb'))\n",
    "df = {'dy':[], 'D':[], 'Y':[]}\n",
    "for dy in ss_dy.keys():\n",
    "    point, test, inf_dict, path, (Xset, Zset) = ss_dy[dy]\n",
    "    d, y = '_'.join(dy.split('_')[:-1]), dy.split('_')[-1]\n",
    "    W, _, W_feats, X, X_binary, X_feats, Z, Z_binary, Z_feats, Y, D = load_ukbb_data(D_label=d, Y_label=y, pp = False)\n",
    "    inf_idxs = inf_dict['switch_sign']\n",
    "    infY, ninfY = Y[inf_idxs].mean(),Y[np.setdiff1d(np.arange(D.shape[0]), inf_idxs)].mean()\n",
    "    infD, ninfD = D[inf_idxs].mean(),D[np.setdiff1d(np.arange(D.shape[0]), inf_idxs)].mean()\n",
    "    df['dy'].append(dy)\n",
    "    df['Y'].append(f'{round(infY*100)}%, {round(ninfY*100)}%')\n",
    "    df['D'].append(f'{round(infD*100)}%, {round(ninfD*100)}%')\n",
    "pd.DataFrame(df)\n",
    "#     Z = Z[:,~bad_idx][:,Zset]\n",
    "#     X = X[:,Xset]\n",
    "#     Z_feats, X_feats = Z_feats[~bad_idx][Zset], X_feats[Xset]\n",
    "#     data = np.concatenate([W,Z,X,D[:,None],Y], axis=1)\n",
    "#     feats = np.concatenate([W_feats,Z_feats,X_feats,[d],[y]])\n",
    "#     FID = np.concatenate([[x.split('.')[1] for x in feats[:-2]],feats[-2:]])\n",
    "#     data.shape, feats.shape, FID.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in inf_dict.items():\n",
    "    print(k, v.shape)\n",
    "inf_idxs = inf_dict['switch_sign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run single categorical association test\n",
    "cat_assn = []\n",
    "cat_fids = []\n",
    "d1 = data[inf_idxs]\n",
    "d0 = data[np.setdiff1d(np.arange(data.shape[0]), inf_idxs)]\n",
    "d1.shape, d0.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_fids = [] # multiple categorical variables - best treated independently\n",
    "for f in tqdm(np.unique(FID)):\n",
    "    rel_feat_idx = np.where(f == FID)[0]\n",
    "    if len(rel_feat_idx) == 1:\n",
    "        if ((data[:,rel_feat_idx]==0)|(data[:,rel_feat_idx]==1)).all():\n",
    "            binary_fids.append(f)\n",
    "    else:\n",
    "        if f in dy_strs:\n",
    "            binary_fids.append(f)\n",
    "            pass\n",
    "        \n",
    "        idx1_data = d1[:, rel_feat_idx]\n",
    "        idx0_data = d0[:, rel_feat_idx]\n",
    "\n",
    "\n",
    "        # Tests if most data points only have a single response for the feature\n",
    "        is_single_cat = ((idx1_data.sum(axis=1) > 1).mean() < .1) and ((idx0_data.sum(axis=1) > 1).mean() < .1)\n",
    "        if is_single_cat:\n",
    "            chi2, p = cat_sim_test(idx1_data, idx0_data)\n",
    "            cat_fids.append('.' + f)\n",
    "            cat_assn.append([chi2, p])\n",
    "        else:\n",
    "            # multiple categorical variables - best treated independently\n",
    "            print(f, ((idx1_data.sum(axis=1) > 1).mean()), ((idx0_data.sum(axis=1) > 1).mean()))\n",
    "            binary_fids.append(f)\n",
    "\n",
    "cat_assn_df = get_assn_df(cat_fids, cat_assn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = .05 / len(feats)\n",
    "cat_assn_df = get_assn_df(cat_fids, cat_assn).drop_duplicates()\n",
    "import seaborn as sns\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    cat_assn_df.fid = cat_assn_df.fid.map(lambda x: x.split('.')[1])\n",
    "    coding_dict = get_coding_dict(np.unique(cat_assn_df.fid))\n",
    "    for _, row in cat_assn_df.sort_values(by='pval').iterrows():\n",
    "        if row.pval < thresh:\n",
    "            idxs = np.where(row.fid==FID)[0]\n",
    "            print(row.fid)\n",
    "            assert len(idxs) > 1        \n",
    "            class_names = [coding_dict[row.fid][int(i.split('.')[2])] for i in feats[idxs]]\n",
    "            stat, pval = row.stat, row.pval\n",
    "            plot_cat_data(d1[:,idxs], d0[:,idxs], \n",
    "                          fids=class_names,title=f'{row.fid} {row.names}')#':\\nChi2 {round(stat,2)} P={round(pval,6)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_assn = []\n",
    "bin_fids = []\n",
    "cont_assn = []\n",
    "cont_fids = []\n",
    "for i,f in tqdm(enumerate(feats)):\n",
    "    if f in dy_strs or f.split('.')[1] in binary_fids:\n",
    "        stat, pval = chi2_binary(d1[:,i], d0[:,i])\n",
    "        bin_assn.append([stat,pval])\n",
    "        bin_fids.append(f)\n",
    "    elif '.' + f.split('.')[1] not in cat_fids:\n",
    "        stat, pval = cont_sim_test(d1[:,i], d0[:,i])\n",
    "        cont_assn.append([stat,pval])\n",
    "        cont_fids.append(f)\n",
    "\n",
    "bin_assn_df =  get_assn_df(bin_fids, bin_assn)\n",
    "cont_assn_df = get_assn_df(cont_fids, cont_assn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in cont_assn_df.sort_values(by='pval').iterrows():\n",
    "    if row.pval < thresh:\n",
    "        i = np.where(feats==row.fid)[0]\n",
    "        print(d1[:,i].mean(), np.median(d1[:,i]))\n",
    "        print(d0[:,i].mean(), np.median(d0[:,i]))\n",
    "        plot_cont_data(d1[:,i], d0[:,i], title=f'{row.names}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_fids = [f.split('.')[1] for f in bin_assn_df.fid if f.count('.') > 1]\n",
    "coding_dict = get_coding_dict(np.unique(bin_fids))\n",
    "for _, row in bin_assn_df.sort_values(by='pval').iterrows():\n",
    "    if row.pval < thresh:\n",
    "        print(row)\n",
    "        if row.fid.count('.') > 1:\n",
    "            class_id = row.fid.split('.')[2]\n",
    "            class_name = '\\n=' + coding_dict[row.fid.split('.')[1]][int(class_id)]\n",
    "        else:\n",
    "            class_name=''\n",
    "        i = np.where(feats==row.fid)[0]\n",
    "        stat, pval = row.stat, row.pval\n",
    "        plot_cat_data(d1[:,i], d0[:,i], fids=[row.fid],title=f'{row.names}{class_name}')#':\\nChi2 {round(stat,2)} P={round(pval,6)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 7),dpi=90)\n",
    "with sns.axes_style('whitegrid'):\n",
    "    dd={'feat': ['Recurrent severe major depression',  'Recurrent severe major depression', \n",
    "              'Felt tense/restless for\\nseveral days in last 2 weeks',\n",
    "              'Felt tense/restless for\\nseveral days in last 2 weeks',              \n",
    "        'Never felt tired/lethargic\\nin last 2 weeks',\n",
    "        'Never felt tired/lethargic\\nin last 2 weeks',\n",
    "        \n",
    "             'Nervous feelings', 'Nervous feelings',],\n",
    "     'point': [6,2,41,20,32,51,50,24],\n",
    "     'group':['High influence patients', 'Typical patients'] * 4}\n",
    "    sns.set_palette(sns.color_palette([\"#B9E5FA\", \"#219EBC\",\"#FB8500\", \"#FFB703\", \"#FAEDCD\",  \"#3C6E71\", ]))\n",
    "    barplot= sns.barplot(data=dd, x='feat', y='point', hue='group')\n",
    "    for p in barplot.patches:\n",
    "        height = p.get_height()\n",
    "        barplot.text(\n",
    "            x=p.get_x() + p.get_width() / 2,\n",
    "            y=height,\n",
    "            s=f'{int(height)}%',\n",
    "            ha='center',\n",
    "            va='bottom',fontsize=22\n",
    "        )\n",
    "    plt.ylim(0,60)\n",
    "    # Add title and labels\n",
    "    plt.title(f'Feature prevalency for patients with high influence in Obese, Osteoarthritis',fontsize=22)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Prevelancy',fontsize=17)\n",
    "    plt.legend(loc='upper left', fontsize=17)\n",
    "    sns.set_palette(sns.color_palette([\"#B9E5FA\", \"#219EBC\",\"#FB8500\", \"#FFB703\", \"#FAEDCD\",  \"#3C6E71\", ]))\n",
    "    plt.xticks(fontsize=17)  # Adjust the fontsize value as needed\n",
    "    plt.yticks(fontsize=17)  # Adjust the fontsize value as needed\n",
    "    plt.show()\n",
    "\n",
    "# 'General pain for 3+ months'\n",
    "# [28, 1]\n",
    "# 'Unable to work because of sickness or disability'\n",
    "# [19, 3]\n",
    "# 'Loneliness, isolation'\n",
    "# [36, 18]\n",
    "# 'Obtained university degree'\n",
    "# [33 19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cat_data(dataA, dataB, fids, title='', labelA='High influence points', labelB='Typical points'):\n",
    "\n",
    "    combined = get_cat_df_from_1hot(dataA, dataB, feats=fids, labelA=labelA, labelB=labelB)\n",
    "    with sns.axes_style(\"whitegrid\"):\n",
    "\n",
    "        plt.figure(figsize=(4*len(fids),3), dpi=100)\n",
    "        relative_freq = combined.groupby(['Dataset', 'Category']).size().reset_index(name='Count')\n",
    "        total_counts = relative_freq.groupby('Dataset')['Count'].transform('sum')\n",
    "        relative_freq['Density'] = relative_freq['Count'] / total_counts\n",
    "        sns.set_palette(sns.color_palette([\"#B9E5FA\", \"#219EBC\",\"#FB8500\", \"#FFB703\", \"#FAEDCD\",  \"#3C6E71\", ]))\n",
    "\n",
    "        barplot= sns.barplot(data=relative_freq, x='Category', y='Density', hue='Dataset')\n",
    "        for p in barplot.patches:\n",
    "            height = p.get_height()\n",
    "            barplot.text(\n",
    "                x=p.get_x() + p.get_width() / 2,\n",
    "                y=height,\n",
    "                s=f'{height*100:.0f}%',\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "            )\n",
    "\n",
    "        # Add title and labels\n",
    "        plt.title(f'{title}')\n",
    "        plt.xlabel('Category')\n",
    "        plt.ylabel('Count')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        sns.set_palette(sns.color_palette([\"#B9E5FA\", \"#219EBC\",\"#FB8500\", \"#FFB703\", \"#FAEDCD\",  \"#3C6E71\", ]))\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency, ks_2samp\n",
    "# from utils import * \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "def get_names():\n",
    "    names = [] \n",
    "    for var in ['sc', 'mc', 'intg', 'cont']: \n",
    "        names.append(pd.read_csv(f'/oak/stanford/groups/rbaltman/karaliu/bias_detection/cohort_creation/helper_data/{var}.csv', sep='\\t'))\n",
    "        names[-1]['var'] = var\n",
    "    return pd.concat(names)\n",
    "names = get_names()\n",
    "def one_hot_to_category(data, feats):\n",
    "    df = pd.DataFrame(data, columns=feats)\n",
    "    return df, df.idxmax(axis=1).str.replace('Category_', '')\n",
    "       \n",
    "def get_cat_df_from_1hot(dataA, dataB, feats=None, labelA='outlier', labelB='typical'):\n",
    "    assert dataA.shape[1] == dataB.shape[1]\n",
    "    if dataA.shape[1] == 1:\n",
    "        A = pd.DataFrame({'Category': dataA.squeeze().astype(str)})\n",
    "        B = pd.DataFrame({'Category': dataB.squeeze().astype(str)})\n",
    "    else:\n",
    "        if feats is None:\n",
    "            feats = np.arange(dataA.shape[1]).astype(str)\n",
    "        assert len(feats) == dataA.shape[1]\n",
    "        if dataB.shape[0] < 100 or dataA.shape[0] < 100:\n",
    "            print(\"Warning: sparse sample size\")\n",
    "        A, cat = one_hot_to_category(dataA, feats)\n",
    "        A['Category']= cat\n",
    "        B, cat = one_hot_to_category(dataB, feats)\n",
    "        B['Category'] = cat\n",
    "\n",
    "    # Add a column to distinguish the datasets\n",
    "    A['Dataset'] = labelA\n",
    "    B['Dataset'] = labelB\n",
    "    # Combine the datasets\n",
    "    combined = pd.concat([A, B])\n",
    "    return combined\n",
    "\n",
    "def plot_cat_data(dataA, dataB, fids, title='', labelA='High influence points', labelB='Typical points'):\n",
    "\n",
    "    combined = get_cat_df_from_1hot(dataA, dataB, feats=fids, labelA=labelA, labelB=labelB)\n",
    "    with sns.axes_style(\"whitegrid\"):\n",
    "\n",
    "        plt.figure(figsize=(3*len(fids), 3), dpi=120)\n",
    "        relative_freq = combined.groupby(['Dataset', 'Category']).size().reset_index(name='Count')\n",
    "        total_counts = relative_freq.groupby('Dataset')['Count'].transform('sum')\n",
    "        relative_freq['Density'] = relative_freq['Count'] / total_counts\n",
    "        sns.set_palette(sns.color_palette([\"#B9E5FA\", \"#219EBC\",\"#FB8500\", \"#FFB703\", \"#FAEDCD\",  \"#3C6E71\", ]))\n",
    "\n",
    "        barplot= sns.barplot(data=relative_freq, x='Category', y='Density', hue='Dataset')\n",
    "        for p in barplot.patches:\n",
    "            height = p.get_height()\n",
    "            barplot.text(\n",
    "                x=p.get_x() + p.get_width() / 2,\n",
    "                y=height,\n",
    "                s=f'{height*100:.0f}%',\n",
    "                ha='center',\n",
    "                va='bottom'\n",
    "            )\n",
    "\n",
    "        # Add title and labels\n",
    "        plt.title(f'{title}')\n",
    "        plt.xlabel('Category')\n",
    "        plt.ylabel('Count')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        sns.set_palette(sns.color_palette([\"#B9E5FA\", \"#219EBC\",\"#FB8500\", \"#FFB703\", \"#FAEDCD\",  \"#3C6E71\", ]))\n",
    "\n",
    "        plt.show()\n",
    "                  \n",
    "def cat_sim_test(dataA, dataB):\n",
    "    combined = get_cat_df_from_1hot(dataA,dataB)\n",
    "    contingency_table = pd.crosstab(combined['Dataset'], combined['Category'])\n",
    "    chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "    return chi2, p\n",
    "\n",
    "             \n",
    "def get_assn_df(fids, assn):\n",
    "    assn = np.array(assn)\n",
    "    names = [get_dscr_from_fid(f) for f in fids] \n",
    "    df = pd.DataFrame({'pval': np.array(assn)[:,1], \n",
    "                                'stat':np.array(assn)[:,0],\n",
    "                                'fid':fids, 'names': names})\n",
    "    return df.sort_values(by='stat')\n",
    "\n",
    "def get_dscr_from_fid(f):\n",
    "    \"\"\" f must be a valid string D, Y or of the format f.{} \"\"\"\n",
    "    assert f in dy_strs or  f.count('.') > 0, f\"Invalid feature {f}\"\n",
    "    try:\n",
    "        return names[names['Field ID'].astype(int) == int(f.split('.')[1])].Description.iloc[0]\n",
    "    except IndexError as e:\n",
    "        print(f\"Didn't find {f} in names\")\n",
    "        return f\n",
    "\n",
    "def chi2_binary(vector1, vector2):\n",
    "    count_0_vector1 = np.sum(vector1 == 0)\n",
    "    count_1_vector1 = np.sum(vector1 == 1)\n",
    "    count_0_vector2 = np.sum(vector2 == 0)\n",
    "    count_1_vector2 = np.sum(vector2 == 1)\n",
    "\n",
    "    contingency_table = np.array([[count_0_vector1, count_1_vector1],\n",
    "                                  [count_0_vector2, count_1_vector2]])\n",
    "    #pval = 1 when same, chi2 = 0 when same\n",
    "    return chi2_contingency(contingency_table)[:2]\n",
    "\n",
    "\n",
    "def cont_sim_test(dataA, dataB, rm_nan=True):\n",
    "    dataA = dataA.squeeze()\n",
    "    dataB = dataB.squeeze()\n",
    "    assert len(dataA.shape)==1\n",
    "    assert len(dataB.shape)==1\n",
    "    if rm_nan: \n",
    "        vecA, vecB = rm_nan_cont(dataA, dataB)\n",
    "    return ks_2samp(vecA, vecB)\n",
    "\n",
    "def plot_cont_data(dataA, dataB, title='', rm_nan=True, labelA='outlier', labelB='typical'):\n",
    "    dataA = dataA.squeeze()\n",
    "    dataB = dataB.squeeze()\n",
    "    assert len(dataA.shape)==1\n",
    "    assert len(dataB.shape)==1\n",
    "    if rm_nan: \n",
    "        vecA, vecB = rm_nan_cont(dataA, dataB)\n",
    "    else:\n",
    "        vecA, vecB = dataA, dataB\n",
    "    sns.kdeplot(vecA, fill=True,label=labelA)    \n",
    "    sns.kdeplot(vecB, fill=True,label=labelB, bw_adjust=2)\n",
    "    perc = max(np.percentile(vecA,99.9), np.percentile(vecB,99.9))\n",
    "    if (perc - vecB.mean())/vecB.std() > 3:\n",
    "        plt.xlim(right=perc)\n",
    "\n",
    "    # sns.histplot(vecA, fill=True,label=labelA)    \n",
    "    # sns.histplot(vecB, fill=True,label=labelB)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "def rm_nan_cont(v1, v2):\n",
    "    \"\"\"\n",
    "    Removes nan values from vectors but bc we mean-imputed, it just\n",
    "    removes the mode of the longest vector\"\"\"\n",
    "    vals, counts = np.unique(v1, return_counts=True)\n",
    "    i = np.argmax(counts)\n",
    "    nan1 = vals[i]\n",
    "    vals, counts = np.unique(v2, return_counts=True)\n",
    "    i = np.argmax(counts)\n",
    "    nan = nan2 = vals[i]\n",
    "    if int(nan2) == nan2:\n",
    "        print(\"Found discrete mode. Ignoring..\")\n",
    "        return v1, v2\n",
    "    elif nan1 != nan2:\n",
    "        print(\"Nans not the same\", nan1, nan2)\n",
    "        if len(v1) > len(v2):\n",
    "            nan = nan1\n",
    "        else:\n",
    "            nan = nan2\n",
    "    eps = 1e-5\n",
    "    v1 = v1[np.abs(v1 - nan) > eps]\n",
    "    v2 = v2[np.abs(v2 - nan) > eps]\n",
    "\n",
    "    return v1, v2"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "T6FelHd78d8T",
    "57abc1f7-1569-469d-8239-12fce056e875",
    "a482021b-9c5b-4123-b20c-878b2940ee33",
    "M0Zp1JDoZBbi"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "11wI4lKMKFJmVN9v4WAK15TtifoY686Vb",
     "timestamp": 1726017290265
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
