{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "nJSbD4Nr8VNq"
   },
   "source": [
    "## Setup Only for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26091,
     "status": "ok",
     "timestamp": 1726010690747,
     "user": {
      "displayName": "Vasilis Syrganis",
      "userId": "00968929417250286436"
     },
     "user_tz": 420
    },
    "id": "j4wBu9BNomlq",
    "outputId": "1ec53172-b732-4692-c04f-a326c223664b"
   },
   "outputs": [],
   "source": [
    "# prompt: mount drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1726010691150,
     "user": {
      "displayName": "Vasilis Syrganis",
      "userId": "00968929417250286436"
     },
     "user_tz": 420
    },
    "id": "JfB4MISjovTQ",
    "outputId": "fcfe2ab8-befa-4ac7-c6db-47e2fb56f7f0"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/Colab\\ Notebooks/hidden_mediators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 904,
     "status": "ok",
     "timestamp": 1726010692051,
     "user": {
      "displayName": "Vasilis Syrganis",
      "userId": "00968929417250286436"
     },
     "user_tz": 420
    },
    "id": "D5tm-9Fno5Xn",
    "outputId": "08097071-b875-4577-c8bf-9147c356f9bc"
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "MFy1PyP89gR3"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "VGF-ucGhpC5P"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "!pip install -r requirements.txt\n",
    "time.sleep(2)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "id": "0wsNOzNVtonf"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# replace `develop` with `install` if you wont make library code changes\n",
    "!python setup.py develop\n",
    "time.sleep(2)\n",
    "clear_output()\n",
    "# Restart the session after running this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1726010708657,
     "user": {
      "displayName": "Vasilis Syrganis",
      "userId": "00968929417250286436"
     },
     "user_tz": 420
    },
    "id": "zyL1_6L73Z9e",
    "outputId": "187a0cf7-f7f5-4731-d93b-7ff36c1212de"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/Colab\\ Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "Msy1rHY6fMoU"
   },
   "source": [
    "## Semi-Synthetic Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from joblib import Parallel, delayed\n",
    "from proximalde.ukbb_proximal import ProximalDE_UKBB, residualizeW_ukbb\n",
    "from proximalde.proximal import ProximalDE, residualizeW, svd_critical_value\n",
    "from proximalde.utilities import covariance\n",
    "from proximalde.gen_data import SemiSyntheticGenerator\n",
    "from sklearn.linear_model import LogisticRegressionCV, LassoCV\n",
    "from proximalde.ukbb_data_utils import *\n",
    "from proximalde.gen_data import gen_data_with_mediator_violations, gen_data_no_controls_discrete_m, gen_data_no_controls, gen_data_complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_label = 'Obese'\n",
    "Y_label = 'back'\n",
    "W, W_binary, W_feats, X, X_binary, X_feats, Z, Z_binary, Z_feats, Y, D = load_ukbb_data(D_label=D_label, Y_label=Y_label)\n",
    "\n",
    "_, X_feats, _, Z_feats = load_ukbb_XZ_data()\n",
    "Xint = get_int_feats(X_feats)\n",
    "Zint = get_int_feats(Z_feats)\n",
    "\n",
    "Dres, Zres, Xres, Yres = residualizeW_ukbb(W, D, Z, X, Y, D_label=D_label, Y_label=Y_label, save_fname_addn='', random_state=3, cv=3)[:4] \n",
    "bad_idx = np.array([('Do not know' in x) or ('Prefer not to' in x) for x in Zint])\n",
    "Zres = Zres[:,~bad_idx]\n",
    "Zint = Zint[~bad_idx]\n",
    "Z = Z[:, ~bad_idx]\n",
    "Z_binary = Z_binary[~bad_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 50000\n",
    "a = 1.0  # a*b is the indirect effect through mediator\n",
    "b = 1.0\n",
    "c = .5  # this is the direct effect we want to estimate\n",
    "d = .0  # this can be zero; does not hurt\n",
    "e = 1.0  # if the product of e*f is small, then we have a weak instrument\n",
    "f = 1.0  # if the product of e*f is small, then we have a weak instrument\n",
    "g = .0  # this can be zero; does not hurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 623,
     "status": "ok",
     "timestamp": 1726019755909,
     "user": {
      "displayName": "Vasilis Syrganis",
      "userId": "00968929417250286436"
     },
     "user_tz": 420
    },
    "id": "xq0f9gwkkBUz",
    "outputId": "085d37c2-c085-4c7d-ae69-4ba76f68e441"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "generator = SemiSyntheticGenerator(split=True)\n",
    "generator.fit(W, D, Z, X, Y, ZXYres=[Zres, Xres, Yres],propensity=np.load('propensity.npy'))\n",
    "# np.save('propensity.npy', generator.propensity_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=np.random.choice(np.arange(len(generator.Xepsilon_)),size=50000).astype(int)\n",
    "sns.histplot(generator.Xepsilon_[idx,:5])\n",
    "plt.show()\n",
    "sns.histplot(generator.Zepsilon_[idx,:5])\n",
    "plt.show()\n",
    "sns.heatmap(covariance(Z, X))\n",
    "plt.show()\n",
    "sns.heatmap(covariance(Ztilde, Xtilde))\n",
    "plt.show()\n",
    "sns.heatmap(covariance(Z, Z))\n",
    "plt.show()\n",
    "sns.heatmap(covariance(Ztilde, Ztilde))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wtilde, Dtilde, _, Ztilde, Xtilde, Ytilde = generator.sample(nsamples, a, b, c, g, replace=True)\n",
    "\n",
    "num_plots=10\n",
    "idx=np.random.choice(np.arange(Z.shape[1]),size=50000).astype(int)\n",
    "fig, axes = plt.subplots(1, num_plots, figsize=(num_plots*4, 4))\n",
    "for i in range(num_plots):\n",
    "    axes[i].hist(Ztilde[:, i], bins=30, alpha=0.5, label=f'sampled', color='blue',density=True)\n",
    "    axes[i].hist(Z[:, i], alpha=0.5, label=f'true', color='orange',density=True)\n",
    "\n",
    "\n",
    "    # Add legend and title\n",
    "    axes[i].legend()\n",
    "    axes[i].set_title(f'Feature {i}')\n",
    "    axes[i].set_yticks([])\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "\n",
    "\n",
    "num_plots=10\n",
    "idx=np.random.choice(np.arange(X.shape[1]),size=50000).astype(int)\n",
    "fig, axes = plt.subplots(1, num_plots, figsize=(num_plots*4, 4))\n",
    "for i in range(num_plots):\n",
    "    axes[i].hist(Xtilde[:, i], bins=30, alpha=0.5, label=f'sampled', color='blue',density=True)\n",
    "    axes[i].hist(X[:, i], alpha=0.5, label=f'true', color='orange',density=True)\n",
    "\n",
    "\n",
    "    # Add legend and title\n",
    "    axes[i].legend()\n",
    "    axes[i].set_title(f'Feature {i}')\n",
    "    axes[i].set_yticks([])\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as stm\n",
    "\n",
    "def minmax(M):\n",
    "    Mmin =  M.min(axis=0,keepdims=True)\n",
    "    Mmax =  M.max(axis=0,keepdims=True)\n",
    "    zero_divide = (Mmin == Mmax).squeeze()\n",
    "    M = (M - Mmin) / (Mmax - Mmin)\n",
    "    if zero_divide.sum():\n",
    "        M[:, zero_divide] = Mmin.squeeze()[zero_divide]+.5\n",
    "    return M\n",
    "\n",
    "def make_binary_Z(Z,sample=False):\n",
    "    if sample:\n",
    "        Z[:,Z_binary] = minmax(Z[:,Z_binary])\n",
    "        Z[:,Z_binary] = np.concatenate(Parallel(n_jobs=-1, verbose=3)(delayed(lambda v: np.random.binomial(1, v))(Z[:,i])\n",
    "                                          for i in np.argwhere(Z_binary)),axis=1)\n",
    "    else:\n",
    "        Z[:, Z_binary] =  (Z[:, Z_binary]> 0).astype(int)\n",
    "    return Z\n",
    "\n",
    "def best_baseline_est(it, generator, n, a, b, c, g, *, sy=1.0, n_jobs=-1, verbose=0, make_binary=True):\n",
    "    np.random.seed(it)\n",
    "    \n",
    "    #binary ignored\n",
    "    \n",
    "    # Z doesn't matter\n",
    "    W, D, M, _, X, Y = generator.sample(n, a, b, c, g, sy=sy, replace=True)\n",
    "\n",
    "    res = stm.OLS(Y, np.hstack([D.reshape(-1, 1), M, X, W, np.ones((D.shape[0], 1))])).fit(cov_type='HC1')\n",
    "    return res.params[0], np.sqrt(res.cov_params()[0, 0])\n",
    "\n",
    "def known_baseline_est(it, generator, n, a, b, c, g, *, sy=1.0, n_jobs=-1, verbose=0, make_binary=True):\n",
    "    np.random.seed(it)\n",
    "\n",
    "    \n",
    "    # M doesn't matter\n",
    "    W, D, _, Z, X, Y = generator.sample(n, a, b, c, g, sy=sy, replace=True)\n",
    "    if make_binary:\n",
    "        Z = make_binary_Z(Z)\n",
    "        print(\"making binary\")\n",
    "    res = stm.OLS(Y, np.hstack([D.reshape(-1, 1), Z, X, W, np.ones((D.shape[0], 1))])).fit(cov_type='HC1')\n",
    "    return res.params[0], np.sqrt(res.cov_params()[0, 0])\n",
    "\n",
    "def lasso_baseline_est(it, generator, n, a, b, c, g, *, sy=1.0, n_jobs=-1, verbose=0, make_binary=True,use_sklearn=False):\n",
    "    np.random.seed(it)\n",
    "\n",
    "    \n",
    "    # M doesn't matter\n",
    "    W, D, _, Z, X, Y = generator.sample(n, a, b, c, g, sy=sy, replace=True)\n",
    "    if make_binary:\n",
    "        Z = make_binary_Z(Z)\n",
    "        print(\"making binary\")\n",
    "    data = np.hstack([D.reshape(-1, 1), Z, X, W, np.ones((D.shape[0], 1))])\n",
    "    if not use_sklearn:\n",
    "        model = stm.OLS(Y, data).fit_regularized(method='elastic_net', alpha=0., L1_wt=1e-3)\n",
    "        return model.params[0]\n",
    "    else:\n",
    "        model = LassoCV(random_state=0, cv=5,n_jobs=-1)\n",
    "        model.fit(data,Y)    \n",
    "    return model.coef_.squeeze()[0]\n",
    "\n",
    "def proximal_est(it, generator, n, a, b, c, g, *, sy=1.0, n_splits=3, semi=True,\n",
    "            n_jobs=-1, verbose=0, make_binary=False, sample_binary=False):\n",
    "    np.random.seed(it)\n",
    "    print(it)\n",
    "    # M is unobserved so we omit it from the return variables\n",
    "    Wt, Dt, _, Zt, Xt, Yt = generator.sample(n, a, b, c, g, sy=sy, replace=True)\n",
    " \n",
    "    if make_binary:\n",
    "        Zt = make_binary_Z(Zt,sample=sample_binary)\n",
    "        binary_Z = Z_binary\n",
    "    else:\n",
    "        binary_Z = []\n",
    "    est = ProximalDE(cv=n_splits, semi=True, binary_D=True,\n",
    "                     model_classification='xgb', binary_Z=binary_Z,\n",
    "                     n_jobs=n_jobs, random_state=it, verbose=verbose)\n",
    "    est.fit(Wt, Dt, Zt, Xt, Yt)\n",
    "    weakiv_stat, _, _, weakiv_crit = est.weakiv_test(alpha=0.05)\n",
    "    idstr, _, _, idstr_crit = est.idstrength_violation_test(alpha=0.05)\n",
    "    pval, _, _, pval_crit = est.primal_violation_test(alpha=0.05)\n",
    "    dval, _, _, dval_crit = est.dual_violation_test(alpha=0.05)\n",
    "    lb, ub = est.robust_conf_int(lb=-2, ub=2)\n",
    "    return est.point_, est.stderr_, est.r2D_, est.r2Z_, est.r2X_, est.r2Y_, \\\n",
    "        idstr, idstr_crit, est.point_pre_, est.stderr_pre_, \\\n",
    "        pval, pval_crit, dval, dval_crit, weakiv_stat, weakiv_crit, \\\n",
    "        lb, ub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### With continuous Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for btype, bfn in zip(['best case OLS baseline, Y=OLS(M, X, W, D)', 'known OLS baseline, Y=OLS(X, W, Z, D)'],\n",
    "                     [best_baseline_est, known_baseline_est]):\n",
    "    print(btype)\n",
    "    results_baseline = Parallel(n_jobs=-1, verbose=3)(delayed(bfn)(i, generator, nsamples,\n",
    "                                                              a, b, c, g, n_jobs=1, make_binary=False)\n",
    "                                              for i in range(100))\n",
    "\n",
    "    points, stderrs = map(np.array, zip(*results_baseline))\n",
    "\n",
    "    print(\"Estimation Quality\")\n",
    "    coverage = np.mean((points + 1.96 * stderrs >= c) & (points - 1.96 * stderrs <= c))\n",
    "    rmse = np.sqrt(np.mean((points - c)**2))\n",
    "    bias = np.abs(np.mean(points) - c)\n",
    "    std = np.std(points)\n",
    "    mean_stderr = np.mean(stderrs)\n",
    "    mean_length = np.mean(2 * 1.96 * stderrs)\n",
    "    median_length = np.median(2 * 1.96 * stderrs)\n",
    "    print(f\"Mean point: {np.mean(points):.3f}\")\n",
    "    print(f\"Coverage: {coverage:.3f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"Bias: {bias:.3f}\")\n",
    "    print(f\"Std: {std:.3f}\")\n",
    "    print(f\"Mean CI length: {mean_length:.3f}\")\n",
    "    print(f\"Median CI length: {mean_length:.3f}\")\n",
    "    print(f\"Mean Estimated Stderr: {mean_stderr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### With binary Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = .5\n",
    "for btype, bfn in zip(['best case OLS baseline, Y=OLS(M, X, W, D)', 'known OLS baseline, Y=OLS(X, W, Z, D)'],\n",
    "                     [best_baseline_est, known_baseline_est]):\n",
    "    print(btype)\n",
    "    results_baseline = Parallel(n_jobs=-1, verbose=3)(delayed(bfn)(i, generator, nsamples,\n",
    "                                                              a, b, c, g, n_jobs=1, make_binary=True)\n",
    "                                              for i in range(100))\n",
    "\n",
    "    points, stderrs = map(np.array, zip(*results_baseline))\n",
    "\n",
    "    print(\"Estimation Quality\")\n",
    "    coverage = np.mean((points + 1.96 * stderrs >= c) & (points - 1.96 * stderrs <= c))\n",
    "    rmse = np.sqrt(np.mean((points - c)**2))\n",
    "    bias = np.abs(np.mean(points) - c)\n",
    "    std = np.std(points)\n",
    "    mean_stderr = np.mean(stderrs)\n",
    "    mean_length = np.mean(2 * 1.96 * stderrs)\n",
    "    median_length = np.median(2 * 1.96 * stderrs)\n",
    "    print(f\"Mean point: {np.mean(points):.3f}\")\n",
    "    print(f\"Coverage: {coverage:.3f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"Bias: {bias:.3f}\")\n",
    "    print(f\"Std: {std:.3f}\")\n",
    "    print(f\"Mean CI length: {mean_length:.3f}\")\n",
    "    print(f\"Median CI length: {mean_length:.3f}\")\n",
    "    print(f\"Mean Estimated Stderr: {mean_stderr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = .5\n",
    "btype = 'known OLS baseline, Y=OLS(X, W, Z, D)'\n",
    "for c in [-.5,-.1,.1,.5]:\n",
    "    results_baseline = Parallel(n_jobs=-1, verbose=3)(delayed(known_baseline_est)(i, generator, nsamples,\n",
    "                                                              a, b, c, g, n_jobs=1, make_binary=True)\n",
    "                                              for i in range(100))\n",
    "\n",
    "    points, stderrs = map(np.array, zip(*results_baseline))\n",
    "\n",
    "    print(\"Estimation Quality\")\n",
    "    coverage = np.mean((points + 1.96 * stderrs >= c) & (points - 1.96 * stderrs <= c))\n",
    "    rmse = np.sqrt(np.mean((points - c)**2))\n",
    "    bias = np.abs(np.mean(points) - c)\n",
    "    std = np.std(points)\n",
    "    mean_stderr = np.mean(stderrs)\n",
    "    mean_length = np.mean(2 * 1.96 * stderrs)\n",
    "    median_length = np.median(2 * 1.96 * stderrs)\n",
    "    print(f\"Mean point: {np.mean(points):.3f}\")\n",
    "    print(f\"Coverage: {coverage:.3f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"Bias: {bias:.3f}\")\n",
    "    print(f\"Std: {std:.3f}\")\n",
    "    print(f\"Mean CI length: {mean_length:.3f}\")\n",
    "    print(f\"Median CI length: {mean_length:.3f}\")\n",
    "    print(f\"Mean Estimated Stderr: {mean_stderr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.004 * 1.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(100)):\n",
    "#     results.append(proximal_est(i,generator,nsamples,a,b,c,g,make_binary=True,sample_binary=True, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import os\n",
    "dfs = {'exp':[]}\n",
    "\n",
    "metric_names = ['Mean point $\\hat{\\theta}$', 'CI $\\pm1.96\\sigma$',\n",
    "                'Average CI $\\pm1.96\\sigma_i$',  'Average coverage of $\\theta_0$',\n",
    "       'RMSE from $\\theta_0$', 'Bias', 'Success of (1) Primal', 'Success of (2) Dual', 'Success of (3) $\\E[\\tilde{D}V] \\neq 0$',\n",
    "       'Success of (4) $V$ strength F-test', 'Success of (5) Cov($\\tilde{X},\\tilde{Z}) rank test']\n",
    "for m in metric_names:\n",
    "    dfs[m] = []       \n",
    "for p in [x for x in os.listdir('./results/semisynthetic') if '.pkl' in x]:\n",
    "    if p.split('_')[-1][0]=='c':\n",
    "        c = float(p.split('_')[-1][1:-4])\n",
    "    else:\n",
    "        c = .5\n",
    "    print(c)\n",
    "    results = pk.load(open('./results/semisynthetic/'+p, 'rb'))\n",
    "    p=p.replace('results_DrealFalse_','')\n",
    "    p=p.replace('_ClsfZFalse_SmpZFalse_Clsfxgb_DbinaryFalse', '')\n",
    "    print('\\n\\n',p)\n",
    "    print(len(results))\n",
    "    points_base, stderrs_base, rmseD, rmseZ, rmseX, rmseY, \\\n",
    "    idstr, idstr_crit, points_alt, stderrs_alt, \\\n",
    "    pval, pval_crit, dval, dval_crit, wiv_stat, wiv_crit, \\\n",
    "    rlb, rub = map(np.array, zip(*results))\n",
    "\n",
    "    points = np.array(points_base)\n",
    "    stderrs = np.array(stderrs_base)\n",
    "#     points = np.array(points_alt)\n",
    "#     stderrs = np.array(stderrs_alt)\n",
    "    dfs['exp'].append(p)\n",
    "    for n,stat in zip(metric_names,[np.mean(points), \n",
    "                                    np.var(points)*1.96, \n",
    "                                    np.mean(1.96 * stderrs), \n",
    "                                    np.mean((points + 1.96 * stderrs >= c) & (points - 1.96 * stderrs <= c)),\n",
    "       np.sqrt(np.mean((points - c)**2)),np.abs(np.mean(points) - c), \n",
    "        np.mean(pval<=pval_crit),np.mean(dval<=dval_crit), np.mean(idstr>=idstr_crit), \n",
    "       np.mean(wiv_stat>=wiv_crit),np.mean([1])]):\n",
    "        dfs[n].append(round(stat,2))\n",
    "        print(f\"{n}: {stat:.3f}\")\n",
    "\n",
    "#     print(\"\\nRobust ConfInt Coverage\")\n",
    "#     rcoverage = np.mean((rub >= c) & (rlb <= c))\n",
    "#     print(f\"Robust Coverage: {rcoverage:.3f}\")\n",
    "pd.DataFrame(dfs).to_csv('synth_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "UKBB_DATA_DIR = '/oak/stanford/groups/rbaltman/karaliu/bias_detection/cohort_creation/data/'\n",
    "\n",
    "\n",
    "#     Dlabel_to_fid = {'Female':[31], 'Black':[21000], \n",
    "#                      'Obese': [21002], 'Asian': [21000], \n",
    "#                      'White': [21000], 'Low_inc': [738, 6138, 6146, 4674], \n",
    "#                      'On_dis': [6146], 'No_uni': [6138], \n",
    "#                      'No_priv_insr': [4674]} #fid is the ID # of a feature in UKBB\n",
    "#     keep_W_idx = [int(f.split('.')[1]) not in Dlabel_to_fid[D_label] for f in all_D_feats]\n",
    "#     W = np.concatenate([W, all_D_data[:,keep_W_idx]], axis=1)\n",
    "#     W_binary = np.concatenate([W_binary, all_D_binary[keep_W_idx]])\n",
    "#     W_feats = np.concatenate([W_feats, all_D_feats[keep_W_idx]])\n",
    "#     return W, W_binary, W_feats\n",
    "\n",
    "# Y = pd.read_csv(UKBB_DATA_DIR + 'updated_Y_labels.csv')\n",
    "Y = pd.read_csv(UKBB_DATA_DIR + 'updated_sa_df_pp.csv')\n",
    "\n",
    "Y = Y[Y.columns[1:]]\n",
    "import seaborn as sns\n",
    "plt.subplots(figsize=(20,20), dpi=80)\n",
    "corr = Y.corr()\n",
    "sns.heatmap(np.abs(corr), cmap=\"Blues\", annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "Msy1rHY6fMoU"
   },
   "source": [
    "## Semi-Synthetic Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from joblib import Parallel, delayed\n",
    "from proximalde.proximal import ProximalDE, residualizeW, svd_critical_value\n",
    "from proximalde.utilities import covariance\n",
    "from proximalde.gen_data import gen_data_with_mediator_violations, gen_data_no_controls_discrete_m, gen_data_no_controls, gen_data_complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1.0  # a*b is the indirect effect through mediator\n",
    "b = 1.0\n",
    "c = .5  # this is the direct effect we want to estimate\n",
    "d = .0  # this can be zero; does not hurt\n",
    "e = 1.0  # if the product of e*f is small, then we have a weak instrument\n",
    "f = 1.0  # if the product of e*f is small, then we have a weak instrument\n",
    "g = .0  # this can be zero; does not hurt\n",
    "\n",
    "n = 100000\n",
    "pw = 10\n",
    "pz, px = 4, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(124)\n",
    "# W, D, _, Z, X, Y = gen_data_with_mediator_violations(n, pw, pz, px, a, b, c, d, e, f, g)\n",
    "# W = None\n",
    "\n",
    "# W, D, _, Z, X, Y = gen_data_complex(n, pw, pz, px, a, b, c, d, e, f, g)\n",
    "\n",
    "## for no controls un-comment this\n",
    "# _, D, _, Z, X, Y = gen_data_no_controls(n, pw, pz, px, a, b, c, d, e, f, g)\n",
    "# W = None\n",
    "\n",
    "## for multi-dimensional mediator uncomment this\n",
    "pm = 2\n",
    "full_rank = False\n",
    "while not full_rank:\n",
    "    E = np.random.normal(0, 2, (pm, pz))\n",
    "    F = np.random.normal(0, 2, (pm, px))\n",
    "    if (np.linalg.matrix_rank(E, tol=0.5) == pm) and (np.linalg.matrix_rank(F, tol=0.5) == pm):\n",
    "        full_rank = True\n",
    "W, D, _, Z, X, Y = gen_data_no_controls_discrete_m(n, pw, pz, px, a, b, c, d, e*E, f*F, g, pm=pm)\n",
    "W = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 623,
     "status": "ok",
     "timestamp": 1726019755909,
     "user": {
      "displayName": "Vasilis Syrganis",
      "userId": "00968929417250286436"
     },
     "user_tz": 420
    },
    "id": "xq0f9gwkkBUz",
    "outputId": "085d37c2-c085-4c7d-ae69-4ba76f68e441"
   },
   "outputs": [],
   "source": [
    "from proximalde.gen_data import SemiSyntheticGenerator\n",
    "\n",
    "a = 1.0  # a*b is the indirect effect through mediator\n",
    "b = 1.0\n",
    "c = .5  # this is the direct effect we want to estimate\n",
    "g = .0  # this can be zero; does not hurt\n",
    "sm = 2.0  # strength of mediator noise; needs to be non-zero for identifiability; only used when pm=1.\n",
    "nsamples = 100000\n",
    "\n",
    "generator = SemiSyntheticGenerator(split=True)\n",
    "generator.fit(W, D, Z, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "sns.histplot(generator.Xepsilon_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wtilde, Dtilde, _, Ztilde, Xtilde, Ytilde = generator.sample(nsamples, a, b, c, g, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(covariance(Z, X))\n",
    "plt.show()\n",
    "sns.heatmap(covariance(Ztilde, Xtilde))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(covariance(Z, Z))\n",
    "plt.show()\n",
    "sns.heatmap(covariance(Ztilde, Ztilde))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Ztilde[:, 0], label='sampled')\n",
    "plt.hist(Z[:, 0], label='true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as stm\n",
    "def exp_res(it, generator, n, a, b, c, g, *, sy=1.0, n_jobs=-1, verbose=0):\n",
    "    np.random.seed(it)\n",
    "\n",
    "    # M is unobserved so we omit it from the return variables\n",
    "    W, D, M, Z, X, Y = generator.sample(n, a, b, c, g, sy=sy, replace=True)\n",
    "\n",
    "    res = stm.OLS(Y, np.hstack([D.reshape(-1, 1), M, X, np.ones((D.shape[0], 1))])).fit(cov_type='HC1')\n",
    "    return res.params[0], np.sqrt(res.cov_params()[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_res(5, generator, nsamples, a, b, c, g, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs=-1, verbose=3)(delayed(exp_res)(i, generator, nsamples,\n",
    "                                                          a, b, c, g, n_jobs=1)\n",
    "                                          for i in range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "points, stderrs = map(np.array, zip(*results))\n",
    "\n",
    "print(\"Estimation Quality\")\n",
    "coverage = np.mean((points + 1.96 * stderrs >= c) & (points - 1.96 * stderrs <= c))\n",
    "rmse = np.sqrt(np.mean((points - c)**2))\n",
    "bias = np.abs(np.mean(points) - c)\n",
    "std = np.std(points)\n",
    "mean_stderr = np.mean(stderrs)\n",
    "mean_length = np.mean(2 * 1.96 * stderrs)\n",
    "median_length = np.median(2 * 1.96 * stderrs)\n",
    "print(f\"Coverage: {coverage:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"Bias: {bias:.3f}\")\n",
    "print(f\"Std: {std:.3f}\")\n",
    "print(f\"Mean CI length: {mean_length:.3f}\")\n",
    "print(f\"Median CI length: {mean_length:.3f}\")\n",
    "print(f\"Mean Estimated Stderr: {mean_stderr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1726019756920,
     "user": {
      "displayName": "Vasilis Syrganis",
      "userId": "00968929417250286436"
     },
     "user_tz": 420
    },
    "id": "FxzsskIclpoh",
    "outputId": "0061e9f0-48c9-4b7b-d0e1-41852628b177"
   },
   "outputs": [],
   "source": [
    "Wtilde, Dtilde, _, Ztilde, Xtilde, Ytilde = generator.sample(nsamples, a, b, c, g, replace=True)\n",
    "\n",
    "# we find that the dual violation still exists, causing a slight bias (the true\n",
    "# value we should recover is c)\n",
    "est = ProximalDE(cv=3, semi=True, n_jobs=-1, random_state=3, verbose=3)\n",
    "est.fit(Wtilde, Dtilde, Ztilde, Xtilde, Ytilde)\n",
    "est.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "executionInfo": {
     "elapsed": 1105,
     "status": "ok",
     "timestamp": 1726019762625,
     "user": {
      "displayName": "Vasilis Syrganis",
      "userId": "00968929417250286436"
     },
     "user_tz": 420
    },
    "id": "HK9XULD-wTAt"
   },
   "outputs": [],
   "source": [
    "def exp_res(it, generator, n, a, b, c, g, *, sy=1.0,\n",
    "            dual_type='Z', ivreg_type='adv', n_splits=3, semi=True,\n",
    "            n_jobs=-1, verbose=0):\n",
    "    np.random.seed(it)\n",
    "\n",
    "    # M is unobserved so we omit it from the return variables\n",
    "    W, D, _, Z, X, Y = generator.sample(n, a, b, c, g, sy=sy, replace=True)\n",
    "\n",
    "    est = ProximalDE(cv=n_splits, semi=semi,\n",
    "                     dual_type=dual_type, ivreg_type=ivreg_type,\n",
    "                     n_jobs=n_jobs, random_state=it, verbose=verbose)\n",
    "    est.fit(W, D, Z, X, Y)\n",
    "    weakiv_stat, _, _, weakiv_crit = est.weakiv_test(alpha=0.05)\n",
    "    idstr, _, _, idstr_crit = est.idstrength_violation_test(alpha=0.05)\n",
    "    pval, _, _, pval_crit = est.primal_violation_test(alpha=0.05)\n",
    "    dval, _, _, dval_crit = est.dual_violation_test(alpha=0.05)\n",
    "    lb, ub = est.robust_conf_int(lb=-2, ub=2)\n",
    "    return est.point_, est.stderr_, est.r2D_, est.r2Z_, est.r2X_, est.r2Y_, \\\n",
    "        idstr, idstr_crit, est.point_pre_, est.stderr_pre_, \\\n",
    "        pval, pval_crit, dval, dval_crit, weakiv_stat, weakiv_crit, \\\n",
    "        lb, ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3820,
     "status": "ok",
     "timestamp": 1726019766814,
     "user": {
      "displayName": "Vasilis Syrganis",
      "userId": "00968929417250286436"
     },
     "user_tz": 420
    },
    "id": "PF4tZpZ1uIJp",
    "outputId": "d5048391-9701-435d-c769-ecbe8f4a24e4"
   },
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs=-1, verbose=3)(delayed(exp_res)(i, generator, nsamples,\n",
    "                                                          a, b, c, g,\n",
    "                                                          dual_type='Z', ivreg_type='adv',\n",
    "                                                          n_splits=3, semi=True, n_jobs=1)\n",
    "                                          for i in range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1726019766814,
     "user": {
      "displayName": "Vasilis Syrganis",
      "userId": "00968929417250286436"
     },
     "user_tz": 420
    },
    "id": "tRdBUIxW0XQ1",
    "outputId": "858ecc04-3d9c-4ae7-8df2-7b76b51c04ea"
   },
   "outputs": [],
   "source": [
    "points_base, stderrs_base, rmseD, rmseZ, rmseX, rmseY, \\\n",
    "    idstr, idstr_crit, points_alt, stderrs_alt, \\\n",
    "    pval, pval_crit, dval, dval_crit, wiv_stat, wiv_crit, \\\n",
    "    rlb, rub = map(np.array, zip(*results))\n",
    "\n",
    "points_base = np.array(points_base)\n",
    "stderrs_base = np.array(stderrs_base)\n",
    "points_alt = np.array(points_alt)\n",
    "stderrs_alt = np.array(stderrs_alt)\n",
    "\n",
    "print(\"Estimation Quality\")\n",
    "for name, points, stderrs in [('Debiased', points_base, stderrs_base), ('Regularized', points_alt, stderrs_alt)]:\n",
    "    print(f\"\\n{name} Estimate\")\n",
    "    coverage = np.mean((points + 1.96 * stderrs >= c) & (points - 1.96 * stderrs <= c))\n",
    "    rmse = np.sqrt(np.mean((points - c)**2))\n",
    "    bias = np.abs(np.mean(points) - c)\n",
    "    std = np.std(points)\n",
    "    mean_stderr = np.mean(stderrs)\n",
    "    mean_length = np.mean(2 * 1.96 * stderrs)\n",
    "    median_length = np.median(2 * 1.96 * stderrs)\n",
    "    print(f\"Coverage: {coverage:.3f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"Bias: {bias:.3f}\")\n",
    "    print(f\"Std: {std:.3f}\")\n",
    "    print(f\"Mean CI length: {mean_length:.3f}\")\n",
    "    print(f\"Median CI length: {mean_length:.3f}\")\n",
    "    print(f\"Mean Estimated Stderr: {mean_stderr:.3f}\")\n",
    "    print(f\"Nuisance R^2 (D, Z, X, Y): {np.mean(rmseD):.3f}, {np.mean(rmseZ):.3f}, {np.mean(rmseX):.3f}, {np.mean(rmseY):.3f}\")\n",
    "\n",
    "print(\"\\nRobust ConfInt Coverage\")\n",
    "rcoverage = np.mean((rub >= c) & (rlb <= c))\n",
    "print(f\"Robust Coverage: {rcoverage:.3f}\")\n",
    "\n",
    "print(\"\\nViolations\")\n",
    "for name, stat, crit in [('Id-Strenth', idstr, idstr_crit), ('WeakIV F-test', wiv_stat, wiv_crit)]:\n",
    "    violation = np.mean(stat <= crit)\n",
    "    print(f\"% Violations of {name}: {violation:.3f}\")\n",
    "for name, stat, crit in [('Primal Existence', pval, pval_crit), ('Dual Existence', dval, dval_crit)]:\n",
    "    violation = np.mean(stat >= crit)\n",
    "    print(f\"% Violations of {name}: {violation:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "id": "2sffn7TE0d5L"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "T6FelHd78d8T",
    "57abc1f7-1569-469d-8239-12fce056e875",
    "a482021b-9c5b-4123-b20c-878b2940ee33",
    "M0Zp1JDoZBbi"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "11wI4lKMKFJmVN9v4WAK15TtifoY686Vb",
     "timestamp": 1726017290265
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
