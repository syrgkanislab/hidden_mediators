{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "nJSbD4Nr8VNq"
   },
   "source": [
    "## Setup Only for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26091,
     "status": "ok",
     "timestamp": 1726010690747,
     "user": {
      "displayName": "Vasilis Syrganis",
      "userId": "00968929417250286436"
     },
     "user_tz": 420
    },
    "id": "j4wBu9BNomlq",
    "outputId": "1ec53172-b732-4692-c04f-a326c223664b"
   },
   "outputs": [],
   "source": [
    "# prompt: mount drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1726010691150,
     "user": {
      "displayName": "Vasilis Syrganis",
      "userId": "00968929417250286436"
     },
     "user_tz": 420
    },
    "id": "JfB4MISjovTQ",
    "outputId": "fcfe2ab8-befa-4ac7-c6db-47e2fb56f7f0"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/Colab\\ Notebooks/hidden_mediators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 904,
     "status": "ok",
     "timestamp": 1726010692051,
     "user": {
      "displayName": "Vasilis Syrganis",
      "userId": "00968929417250286436"
     },
     "user_tz": 420
    },
    "id": "D5tm-9Fno5Xn",
    "outputId": "08097071-b875-4577-c8bf-9147c356f9bc"
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "MFy1PyP89gR3"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "VGF-ucGhpC5P"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "!pip install -r requirements.txt\n",
    "time.sleep(2)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "id": "0wsNOzNVtonf"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# replace `develop` with `install` if you wont make library code changes\n",
    "!python setup.py develop\n",
    "time.sleep(2)\n",
    "clear_output()\n",
    "# Restart the session after running this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1726010708657,
     "user": {
      "displayName": "Vasilis Syrganis",
      "userId": "00968929417250286436"
     },
     "user_tz": 420
    },
    "id": "zyL1_6L73Z9e",
    "outputId": "187a0cf7-f7f5-4731-d93b-7ff36c1212de"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/Colab\\ Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "T6FelHd78d8T"
   },
   "source": [
    "# Main Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "a1446bbd-c4be-4846-b09a-e36fad2802ab"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "c79a60f7-dde3-4c7c-8419-ce831b20e824"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from joblib import Parallel, delayed\n",
    "from proximalde.gen_data import gen_data_complex, gen_data_no_controls, gen_data_with_mediator_violations, gen_data_no_controls_discrete_m\n",
    "from proximalde.proximal import proximal_direct_effect, ProximalDE, residualizeW\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from proximalde.crossfit import fit_predict\n",
    "from proximalde.utilities import covariance, svd_critical_value\n",
    "from proximalde.proximal import residualizeW\n",
    "from proximalde.proxy_rm_utils import *\n",
    "from proximalde.ukbb_data_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1.0  # a*b is the indirect effect through mediator\n",
    "b = 1.0\n",
    "c = .5  # this is the direct effect we want to estimate\n",
    "d = .0  # this can be zero; does not hurt\n",
    "e = .7  # if the product of e*f is small, then we have a weak instrument\n",
    "f = .5  # if the product of e*f is small, then we have a weak instrument\n",
    "g = .0  # this can be zero; does not hurt\n",
    "\n",
    "n = 50000\n",
    "pw = 100\n",
    "pz, px = 50, 40\n",
    "invalidZ = [0, 4, 5]\n",
    "invalidX = [0, 6, 8]\n",
    "np.random.seed(0)\n",
    "\n",
    "validZ = np.setdiff1d(np.arange(pz), invalidZ)\n",
    "validX = np.setdiff1d(np.arange(px), invalidX)\n",
    "W, D, _, Z, X, Y = gen_data_with_mediator_violations(n, pw, pz, px, a, b, c, d, e, f, g,\n",
    "                                                     invalidZinds=invalidZ, invalidXinds=invalidX)\n",
    "W = None\n",
    "\n",
    "np.random.seed(0)\n",
    "Dres, Zres, Xres, Yres, *_ = residualizeW(W, D, Z, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "prm = WeakProxyRemoval(Xres,Zres,Dres,primal_type='est')\n",
    "prm.update_Y(Yres)\n",
    "prm.dv_bench, prm.pv_bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "prm.violation(list(validX), list(validZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "prm.violation(list(invalidX), list(invalidZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "prm.dthresh = .08\n",
    "prm.pthresh = .08\n",
    "candidates = prm.find_candidate_sets(10)\n",
    "len(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only shows when # test passes > 2 \n",
    "prm.get_estimates(candidates, verbose=1,npass=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretable X, Z features \n",
    "X, X_feats, Z, Z_feats = load_ukbb_XZ_data()\n",
    "Xint = get_int_feats(X_feats)\n",
    "Zint = get_int_feats(Z_feats)\n",
    "D_label = 'Obese'\n",
    "Y_label = 'back'\n",
    "Xres, Zres, Yres, Dres = load_ukbb_res_data(D_label, Y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "prm = WeakProxyRemoval(Xres,Zres,Dres,primal_type='est')\n",
    "prm.update_Y(Yres)\n",
    "prm.Xint = Xint\n",
    "prm.Zint = Zint\n",
    "prm.dv_bench, prm.pv_bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "prm = WeakProxyRemoval(Xres,Zres,Dres,primal_type='est')\n",
    "prm.update_Y(Yres)\n",
    "prm.Xint = Xint\n",
    "prm.Zint = Zint\n",
    "prm.dv_bench, prm.pv_bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "prm.dthresh = .08\n",
    "prm.pthresh = .08\n",
    "prm.change_primal_type('est')\n",
    "N = 10\n",
    "gen_nextX='random'\n",
    "gen_nextZ='random'\n",
    "\n",
    "np.random.seed(0)\n",
    "candidates = prm.find_candidate_sets(N,gen_nextX=gen_nextX,gen_nextZ=gen_nextZ)\n",
    "print(len(candidates))\n",
    "prm.get_estimates(candidates, idx_list =np.random.choice(np.arange(len(candidates)),size=6), verbose=1, npass=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretable X, Z features \n",
    "X, X_feats, Z, Z_feats = load_ukbb_XZ_data()\n",
    "Xint = get_int_feats(X_feats)\n",
    "Zint = get_int_feats(Z_feats)\n",
    "D_label = 'Obese'\n",
    "Y_label = 'back'\n",
    "Xres, Zres, Yres, Dres = load_ukbb_res_data(D_label, Y_label)\n",
    "bad_idx = np.array([('Do not know' in x) or ('Prefer not to' in x) for x in Zint])\n",
    "prm2 = WeakProxyRemoval(Xres,Zres[:,~bad_idx],Dres,primal_type='est')\n",
    "prm2.update_Y(Yres)\n",
    "prm2.Xint = Xint\n",
    "prm2.Zint = Zint[~bad_idx]\n",
    "prm2.dv_bench, prm2.pv_bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir =f'./results/proxyrm/Obese_back/ntrials200_dth0.1_pth0.1_ptyfull_genXrandom_genZrandom_Rgrs=linear/'\n",
    "candidates = pk.load(open(save_dir + 'candidates.pkl', 'rb'))\n",
    "np.random.seed(2)\n",
    "prm.get_estimates(candidates, idx_list =np.random.choice(np.arange(len(candidates)),size=50), verbose=1, npass=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_feats, Z, Z_feats = load_ukbb_XZ_data()\n",
    "Zint = get_int_feats(Z_feats)\n",
    "Xint = get_int_feats(X_feats)\n",
    "D_label = 'Obese'\n",
    "Y_label = 'back'\n",
    "Xres, Zres, Yres, Dres = load_ukbb_res_data(D_label, Y_label)\n",
    "bad_idx = np.array([('Do not know' in x) or ('Prefer not to' in x) for x in Zint])\n",
    "prm2 = WeakProxyRemoval(Xres,Zres[:,~bad_idx],Dres,primal_type='est')\n",
    "prm2.update_Y(Yres)\n",
    "prm2.Xint = Xint\n",
    "prm2.Zint = Zint[~bad_idx]\n",
    "prm2.dv_bench, prm2.pv_bench\n",
    "N = 25\n",
    "gen_nextX='random'\n",
    "gen_nextZ='random'\n",
    "\n",
    "np.random.seed(0)\n",
    "candidates = prm2.find_candidate_sets(N,gen_nextX=gen_nextX,gen_nextZ=gen_nextZ)\n",
    "print(len(candidates))\n",
    "prm2.get_estimates(candidates, idx_list =np.random.choice(np.arange(len(candidates)),size=40), verbose=1, npass=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "prm2.Zint = Zint[~bad_idx]\n",
    "N = 20\n",
    "gen_nextX='random'\n",
    "gen_nextZ='random'\n",
    "\n",
    "np.random.seed(0)\n",
    "candidates = prm2.find_candidate_sets(N,gen_nextX=gen_nextX,gen_nextZ=gen_nextZ)\n",
    "print(len(candidates))\n",
    "prm2.get_estimates(candidates, idx_list =np.random.choice(np.arange(len(candidates)),size=40), verbose=1, npass=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "for D_label in ['Female','Obese','Black', 'Asian']:\n",
    "    for Y_label in tqdm(['OA', 'myoc', 'back']):\n",
    "        dy=f'{D_label}_{Y_label}'\n",
    "        print(\"\\n\"*3)\n",
    "        print(dy)\n",
    "        dir = f'./results/D={D_label}_Y={Y_label}/Dbin=False_Ybin=False_XZbin=False_Rgr=linear/'\n",
    "        test = pd.read_csv(dir + '/table2.csv',header=1, index_col=1)\n",
    "        point = pd.read_csv(dir + '/table0.csv',header=1, index_col=1)\n",
    "        print(\"Estimation on all data:\")\n",
    "        display(point)\n",
    "        display(test)\n",
    "        dir = f'./results/proxyrm/{dy}'\n",
    "        all_paths = os.listdir(dir)\n",
    "        import seaborn as sns\n",
    "        all_data = {}\n",
    "        print(\"Estimations after rm weak proxies (that pass >2 tests):\")\n",
    "\n",
    "        for var in all_paths:\n",
    "            print(var)\n",
    "            est_paths = [x for x in os.listdir(f'{dir}/{var}') if not '.pkl' in x]\n",
    "            pass_tests = []\n",
    "            for est in est_paths:\n",
    "                test = pd.read_csv(f'{dir}/{var}/{est}/table2.csv',header=1, index_col=1)\n",
    "                if test['pass test'].sum() > 2:\n",
    "                    point = pd.read_csv(f'{dir}/{var}/{est}/table0.csv',header=1, index_col=1)\n",
    "#                     display(point)\n",
    "#                     display(test)\n",
    "#                     print(\"-\"*20)\n",
    "                pass_tests.append(test['pass test'].sum())\n",
    "#             sns.histplot(pass_tests)\n",
    "            print((np.array(pass_tests)>2).mean())\n",
    "#             if (np.array(pass_tests)==4).any():\n",
    "#                 print(1/0)\n",
    "#             plt.show()\n",
    "\n",
    "        dir = f'./results/proxyrm/old_est/'\n",
    "        all_paths = [p for p in os.listdir(dir) if dy in p]\n",
    "        pass_tests = []\n",
    "        for est in all_paths:\n",
    "            try:\n",
    "                test = pd.read_csv(f'{dir}/{est}/table2.csv',header=1, index_col=1)\n",
    "                pass_tests.append(test['pass test'].sum())\n",
    "                if test['pass test'].sum() > 2:\n",
    "                    point = pd.read_csv(f'{dir}/{est}/table0.csv',header=1, index_col=1)\n",
    "                    display(point)\n",
    "                    display(test)\n",
    "#                 if test['pass test'].sum() == 3:\n",
    "#                     print(est)\n",
    "            except:\n",
    "                pass\n",
    "#         print((np.array(pass_tests)>2).mean())\n",
    "#         sns.histplot(pass_tests)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk \n",
    "np.random.seed(30)\n",
    "for D_label in ['Female','Obese','Black', 'Asian']:\n",
    "    for Y_label in tqdm(['OA', 'myoc', 'back']):\n",
    "        dy=f'{D_label}_{Y_label}'\n",
    "        dy = 'Obese_back'\n",
    "        print(\"\\n\"*3)\n",
    "        print(dy)\n",
    "        dir = f'./results/D={D_label}_Y={Y_label}/Dbin=False_Ybin=False_XZbin=False_Rgr=linear/'\n",
    "        test = pd.read_csv(dir + '/table2.csv',header=1, index_col=1)\n",
    "        point = pd.read_csv(dir + '/table0.csv',header=1, index_col=1)\n",
    "        print(\"Estimation on all data:\")\n",
    "        display(point)\n",
    "        display(test)\n",
    "        dir = f'./results/proxyrm/{dy}'\n",
    "        all_paths = os.listdir(dir)\n",
    "        import seaborn as sns\n",
    "        all_data = {}\n",
    "        print(\"Estimations after rm weak proxies (that pass >2 tests):\")\n",
    "        xs = []\n",
    "        zs = []\n",
    "        points = []\n",
    "        for var in all_paths:\n",
    "#             print(var)\n",
    "            candidates = pk.load(open(f'{dir}/{var}/candidates.pkl', 'rb'))\n",
    "            est_paths = [x for x in os.listdir(f'{dir}/{var}') if not '.pkl' in x]\n",
    "            pass_tests = []\n",
    "            for est in est_paths:\n",
    "                test = pd.read_csv(f'{dir}/{var}/{est}/table2.csv',header=1, index_col=1)\n",
    "                if test['pass test'].sum() > 2:\n",
    "                    point = pd.read_csv(f'{dir}/{var}/{est}/table0.csv',header=1, index_col=1)\n",
    "                    xset, zset = candidates[int(est.split('_')[-1])]\n",
    "                    print(len(xset), len(zset))\n",
    "                    xset_ = np.zeros(65)\n",
    "                    zset_ = np.zeros(197)\n",
    "                    xset_[xset] = 1\n",
    "                    zset_[zset] = 1\n",
    "                    xs.append(xset_[None,:])\n",
    "                    zs.append(zset_[None,:])\n",
    "                    display(point)\n",
    "                    display(test)\n",
    "                    print(\"-\"*20)\n",
    "                    points.append(point.point.iloc[0])\n",
    "                pass_tests.append(test['pass test'].sum())\n",
    "#             sns.histplot(pass_tests)\n",
    "#             print((np.array(pass_tests)>2).mean())\n",
    "#             if (np.array(pass_tests)==4).any():\n",
    "#                 print(1/0)\n",
    "#             plt.show()\n",
    "        print(1/0)\n",
    "        dir = f'./results/proxyrm/old_est/'\n",
    "        all_paths = [p for p in os.listdir(dir) if dy in p]\n",
    "        pass_tests = []\n",
    "        for est in all_paths:\n",
    "            try:\n",
    "                test = pd.read_csv(f'{dir}/{est}/table2.csv',header=1, index_col=1)\n",
    "                pass_tests.append(test['pass test'].sum())\n",
    "                if test['pass test'].sum() > 2:\n",
    "                    point = pd.read_csv(f'{dir}/{est}/table0.csv',header=1, index_col=1)\n",
    "                    display(point)\n",
    "                    display(test)\n",
    "#                 if test['pass test'].sum() == 3:\n",
    "#                     print(est)\n",
    "            except:\n",
    "                pass\n",
    "#         print((np.array(pass_tests)>2).mean())\n",
    "#         sns.histplot(pass_tests)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.concatenate(xs)\n",
    "xsp = xs[np.array(points)>0]\n",
    "xsn = xs[np.array(points)<0]\n",
    "diff = xsp.mean(axis=0) - xsn.mean(axis=0)\n",
    "idx = np.argsort(np.abs(diff))[::-1]\n",
    "print(diff[idx])\n",
    "Xint[idx] \n",
    "# positive numbers means including that Xfeature tends to create a positive bias effect (more likely to be diag)\n",
    "# negative numbers mean including that Xfeature ends to create a more negative effect (less likely to be diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Most important features for passing some tests - caution bc depends on Z\n",
    "sm = xs.sum(axis=0)\n",
    "idx1 = np.argsort(np.abs(sm))[::-1]\n",
    "print(sm[idx1])\n",
    "Xint[idx1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = np.concatenate(zs)[:,:196]\n",
    "xsp = zs[np.array(points)>0]\n",
    "xsn = zs[np.array(points)<0]\n",
    "diff = xsp.mean(axis=0) - xsn.mean(axis=0)\n",
    "idx = np.argsort(np.abs(diff))[::-1]\n",
    "print(diff[idx])\n",
    "Zint[idx] \n",
    "# positive numbers means including that Zfeature tends to create a positive bias effect (more likely to be diag)\n",
    "# negative numbers mean including that Zfeature ends to create a more negative effect (less likely to be diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Most important features for passing some tests - caution bc depends on X\n",
    "sm = zs.sum(axis=0)\n",
    "idx1 = np.argsort(np.abs(sm))[::-1]\n",
    "print(sm[idx1])\n",
    "Zint[idx1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "all_proxyrm_table_paths = os.listdir(f'./results/proxyrm/')\n",
    "all_data = {}\n",
    "for D_label in ['Female','Obese','Black', 'Asian']:\n",
    "    for Y_label in tqdm(['deprs', 'back']):\n",
    "        dir = f'./results/D={D_label}_Y={Y_label}/Dbin=False_Ybin=False_XZbin=False_Rgr=linear/'\n",
    "        test = pd.read_csv(dir + '/table2.csv',header=1, index_col=1)\n",
    "        point = pd.read_csv(dir + '/table0.csv',header=1, index_col=1)\n",
    "\n",
    "#         candidates = pk.load(open(f'./{D_label}_{Y_label}_candidates_reweight_acually.pkl', 'rb'))\n",
    "        candidates = pk.load(open(f'./{D_label}_{Y_label}_candidates_reweight.pkl', 'rb'))\n",
    "        new_tests = []\n",
    "        new_points = []\n",
    "        xsets=[]\n",
    "        zsets=[]\n",
    "        ps=[p for p in all_proxyrm_table_paths if f'{D_label}_{Y_label}' in p and 'Dbin' not in p and 'random' in p]\n",
    "        if len(ps) > 0:\n",
    "            print(dir)\n",
    "            display(point)\n",
    "            display(test)\n",
    "        for path in ps:\n",
    "            try:\n",
    "                point = pd.read_csv(f'./results/proxyrm/{path}/table0.csv', header=1, index_col=1)\n",
    "                test = pd.read_csv(f'./results/proxyrm/{path}/table2.csv', header=1, index_col=1)\n",
    "                i = int(path.split(\"_\")[-1])\n",
    "                Xset, Zset = candidates[i]\n",
    "                if test['pass test'].sum() > 2:\n",
    "                    print(\"----\"*10)\n",
    "                    new_tests.append(test)\n",
    "                    new_points.append(point)\n",
    "                    rmXset = np.setdiff1d(np.arange(Xres.shape[1]), Xset)\n",
    "                    x=Xint[Xset]\n",
    "                    np.random.shuffle(x)\n",
    "                    print(\"Kept Xs = \", x[:10])\n",
    "                    x=Xint[rmXset]\n",
    "                    np.random.shuffle(x)\n",
    "                    print(\"Deleted Xs =\", x)\n",
    "\n",
    "                    rmZset = np.setdiff1d(np.arange(Zres.shape[1]), Zset)\n",
    "                    \n",
    "                    x=Zint[Zset]\n",
    "                    np.random.shuffle(x)\n",
    "                    print(\"Kept Zs = \", x[:10])\n",
    "                    x=Zint[rmZset]\n",
    "                    np.random.shuffle(x)\n",
    "                    print(\"Deleted Zs =\", x[:10])\n",
    "                    x = np.zeros(Xres.shape[1])\n",
    "                    x[Xset] = 1\n",
    "                    z = np.zeros(Zres.shape[1])\n",
    "                    z[Zset] = 1\n",
    "                    xsets.append(x[None,:])\n",
    "                    zsets.append(z[None,:])\n",
    "                    display(test)\n",
    "                    display(point)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        if len(new_tests) > 0:\n",
    "            plt.subplots(figsize=(10,10))\n",
    "            plt.imshow(np.concatenate(xsets))\n",
    "            plt.show()\n",
    "            plt.subplots(figsize=(10,10))\n",
    "            plt.imshow(np.concatenate(zsets))\n",
    "            plt.show()\n",
    "            all_data[f'{D_label}_Y={Y_label}'] = {'tests': [og_test]+new_tests, \n",
    "                                    'point': [og_point] + new_points, 'xsets': xsets,\n",
    "                                                 'zsets':zsets}\n",
    "            print(f'{D_label}_Y={Y_label}', len(new_tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dfs = []\n",
    "res_dfs = []\n",
    "point_dfs = []\n",
    "model_regression = 'linear'\n",
    "model_classification = 'linear'\n",
    "Dbin = False \n",
    "Ybin = False\n",
    "SAVE_PATH = './results/'\n",
    "XZbin = False\n",
    "for D_label in ['Black', 'Female', 'Obese','Asian']:\n",
    "        print(D_label)\n",
    "        for Y_label in ['OA', 'RA', 'myoc','deprs', 'back']:\n",
    "\n",
    "            try:\n",
    "#                     res_model_save = ''\n",
    "#                     if res_model == 'xgb':\n",
    "#                         res_model_save = '_xgb'\n",
    "                save_dir = f'{SAVE_PATH}/D={D_label}_Y={Y_label}/Dbin={Dbin}_Ybin={Ybin}_XZbin={XZbin}_Rgr={model_regression}'\n",
    "                test_df = pd.read_csv(save_dir + '/table2.csv', header=1, index_col=1)\n",
    "                test_df = test_df.drop(columns=['0'])\n",
    "                test_df_flat = test_df.T.unstack().to_frame().sort_index(level=1).T\n",
    "                test_df_flat.columns = test_df_flat.columns.map('_'.join)\n",
    "                point_df = pd.read_csv(save_dir + '/table0.csv', header=1, index_col=1)\n",
    "                point_df = point_df.drop(columns=['0'])                    \n",
    "                res_df = pd.read_csv(save_dir + '/table1.csv', header=1, index_col=1)\n",
    "                res_df = res_df.drop(columns=['0'])\n",
    "                test_df_flat['D_Y'] = point_df['D_Y'] = res_df['D_Y'] = f'{D_label}_{Y_label}'\n",
    "#                 test_df_flat['res_model'] = point_df['res_model'] = res_df['res_model'] = res_model\n",
    "                res_dfs.append(res_df)\n",
    "                point_dfs.append(point_df)\n",
    "                test_dfs.append(test_df_flat)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "point_df = pd.concat(point_dfs)\n",
    "res_df = pd.concat(res_dfs)\n",
    "test_df = pd.concat(test_dfs)\n",
    "test_df = test_df.reindex(sorted(test_df.columns), axis=1)\n",
    "point_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zset_trial2(it, remnantX, verbose):\n",
    "    ''' We try to add elements to the X's in random order, while maintaining that the dual\n",
    "    violation is not violated. Here we use all the Z's, since the dual violation can only\n",
    "    improve if we add more Z's.\n",
    "    '''\n",
    "    np.random.seed(it)\n",
    "    unusedZ = np.arange(Zres.shape[1])\n",
    "    remnantZ = []\n",
    "    while len(unusedZ) > 0:\n",
    "        next = np.random.choice(len(unusedZ), size=1)[0]\n",
    "        dv, pv = violation(remnantX, remnantZ + [unusedZ[next]])\n",
    "        if pv < 0.1 * pv_bench:\n",
    "            remnantZ += [unusedZ[next]]\n",
    "        unusedZ = np.delete(unusedZ, next)\n",
    "\n",
    "    if remnantZ:\n",
    "        remnantZ = np.sort(remnantZ)\n",
    "        if verbose:\n",
    "            print(remnantZ, violation(remnantX, remnantZ))\n",
    "    \n",
    "        ohe = np.zeros(Zres.shape[1]).astype(int)\n",
    "        ohe[remnantZ] = 1\n",
    "        return ohe\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def xset_trial2(it, remnantZ, verbose):\n",
    "    ''' Given a candidate X set, we try to add elements to the Z's in random order,\n",
    "    while maintaining that the primal violation does not occur.\n",
    "    '''\n",
    "    np.random.seed(it)\n",
    "    \n",
    "    unusedX = np.arange(Xres.shape[1])\n",
    "    remnantX = []\n",
    "    while len(unusedX) > 0:\n",
    "        next = np.random.choice(len(unusedX), size=1)[0]\n",
    "        dv, pv = violation(remnantX + [unusedX[next]],remnantZ)\n",
    "        if dv < .1 * dv_bench:\n",
    "            remnantX += [unusedX[next]]\n",
    "        unusedX = np.delete(unusedX, next)\n",
    "\n",
    "    if remnantX:\n",
    "        remnantX = np.sort(remnantX)\n",
    "    \n",
    "        dv, pv = violation(remnantX, remnantZ)\n",
    "        if verbose:\n",
    "            print(remnantX, remnantZ, dv, pv)\n",
    "    \n",
    "        ohe = np.zeros(Xres.shape[1] + Zres.shape[1]).astype(int)\n",
    "        ohe[remnantX] = 1\n",
    "        ohe[Xres.shape[1] + remnantZ] = 1\n",
    "        return ohe\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "from tqdm import tqdm\n",
    "def find_candidate_sets_Xfirst(ntrials, verbose=0, n_jobs=-1, gen_next='random'):\n",
    "    unique_Xsets = np.array([np.ones(Xres.shape[1])]).astype(int)\n",
    "\n",
    "    for _ in range(5):\n",
    "        # we generate a set of candidate of maximal X sets such that the dual violation does not\n",
    "        # occur, when we use all the Z's. Note that more Z's can only help the dual.\n",
    "        candidateZ = []\n",
    "        for remnantX in unique_Xsets:\n",
    "            remnantX = np.argwhere(remnantX).flatten()\n",
    "            candidateZ += Parallel(n_jobs=n_jobs, verbose=3)(delayed(zset_trial2)(it, remnantX, verbose)\n",
    "                                                             for it in range(ntrials))\n",
    "        candidateZ = [c for c in candidateZ if c is not None]\n",
    "\n",
    "        if not candidateZ:\n",
    "            return []\n",
    "\n",
    "        candidateZ = np.array(candidateZ).astype(int)\n",
    "        # we clean up to keep only the unique solutions\n",
    "        unique_Zsets = np.unique(candidateZ, axis=0)\n",
    "    \n",
    "        candidateXZ = []\n",
    "        # for each unique candidate solution of X's\n",
    "        for remnantZ in tqdm(unique_Zsets):\n",
    "            remnantZ = np.argwhere(remnantZ).flatten()\n",
    "            # we try to construct maximal sets of Z's, such that the primal violation\n",
    "            # does not occur. Note that more X's can only help the primal, which is why\n",
    "            # we tried to build maximal X's in the first place.\n",
    "            candidateXZ += Parallel(n_jobs=n_jobs, verbose=3)(delayed(xset_trial2)(it, remnantZ, verbose)\n",
    "                                                              for it in range(ntrials))\n",
    "        candidateXZ = [c for c in candidateXZ if c is not None]\n",
    "\n",
    "        if not candidateXZ:\n",
    "            return []\n",
    "\n",
    "        # this array now contains the one-hot-encodings of the Xset and the Zset (concatenated)\n",
    "        candidateXZ = np.array(candidateXZ).astype(int)\n",
    "        # we clean up to keep only unique Zset solutions\n",
    "        ##THIS MIGHT BE WRONG\n",
    "        unique_Xsets = np.unique(candidateXZ[:, :Xres.shape[1]], axis=0)\n",
    "\n",
    "    # we clean up to keep only unique pairs of solutions\n",
    "    unique_XZsets = np.unique(candidateXZ, axis=0)\n",
    "    # we transform the one hot encodings back to member sets\n",
    "    final_candidates = []\n",
    "    for unique_XZ in unique_XZsets:\n",
    "        Xset = np.argwhere(unique_XZ[:Xres.shape[1]]).flatten()\n",
    "        Zset = np.argwhere(unique_XZ[Xres.shape[1]:]).flatten()\n",
    "        dv, pv = violation(Xset, Zset)\n",
    "        if verbose:\n",
    "            print(Xset, Zset, dv, pv)\n",
    "        if pv < 0.1 * pv_bench and dv < 0.1 * dv_bench:\n",
    "            final_candidates += [(Xset, Zset)]\n",
    "    return final_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "for i in idx_list = np.random.choice(np.arange(len(candidates)), size=2)\n",
    "for Xset, Zset in candidates[idx_list]:\n",
    "    print(\"Xset =\", Xset)\n",
    "    print(\"Zset =\", Zset)\n",
    "    print()\n",
    "    est = ProximalDE(random_state=3)\n",
    "    est.fit(None, Dres, Zres[:, Zset], Xres[:, Xset], Yres)\n",
    "    t = est.summary().tables[2]\n",
    "    df = pd.DataFrame.from_records(t.data)\n",
    "    header = df.iloc[0] # grab the first row for the header\n",
    "    df = df[1:] # take the data less the header row\n",
    "    df.columns = header\n",
    "    df['pass test'] = df['pass test'].map(lambda x: x == 'True')\n",
    "    if df['pass test'].all():\n",
    "        display(est.summary().tables[0], est.summary().tables[2])\n",
    "        print(\"Xset =\", Xset)\n",
    "        print(\"Zset =\", Zset)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "T6FelHd78d8T",
    "57abc1f7-1569-469d-8239-12fce056e875",
    "a482021b-9c5b-4123-b20c-878b2940ee33",
    "M0Zp1JDoZBbi"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "11wI4lKMKFJmVN9v4WAK15TtifoY686Vb",
     "timestamp": 1726017290265
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
